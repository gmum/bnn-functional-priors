{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Et2uW8-N2Pc1"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YB0AjXkk2Pc2"
   },
   "outputs": [],
   "source": [
    "import sys, logging\n",
    "\n",
    "logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        stream=sys.stdout,\n",
    "        format=\"[%(asctime)s] {%(pathname)s:%(lineno)d} %(levelname)s - %(message)s\",\n",
    "        force=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_9ZNBlE52Pc3"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "from scipy.stats import norm\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1qVA_b2L4zof"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kusmierc/bnn-from-gp/bnngp/.env/lib/python3.9/site-packages/torch/__init__.py:955: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:432.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
    "    # torch.set_default_tensor_type(\"torch.cuda.DoubleTensor\")\n",
    "    device = \"cuda\"\n",
    "\n",
    "else:\n",
    "    torch.set_default_tensor_type(\"torch.FloatTensor\")\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.set_default_tensor_type(\"torch.FloatTensor\")\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used device = cuda\n"
     ]
    }
   ],
   "source": [
    "print(f\"Used device = {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "U236a66X2Pc4"
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"../\")\n",
    "\n",
    "import reparameterized\n",
    "from reparameterized import sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Failed to import from rational.torch: No module named 'rational'\n"
     ]
    }
   ],
   "source": [
    "from data.minibatches import generate_minibatches, iterate_over_minibatches\n",
    "from aux.gpu import gpu_memory_info_smi\n",
    "\n",
    "import import_export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XZHqIAWH2Pc5"
   },
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import args as args_loading\n",
    "\n",
    "# used when run from command line\n",
    "args = args_loading.parse_args()\n",
    "# print(f\"Parsing args = {args}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = args.get(\"random_seed\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_proportion = args.get(\"train_data_proportion\", 6 / 9)\n",
    "\n",
    "\n",
    "# NF \n",
    "nf_model = args.get(\"nf_model\", 0) \n",
    "realnvp_rezero = args.get(\"realnvp_rezero\", False)\n",
    "realnvp_num_layers = args.get(\"realnvp_num_layers\", 32)\n",
    "realnvp_m_multiplier = args.get(\"realnvp_m_multiplier\", 6)\n",
    "\n",
    "# NF posterior training\n",
    "stl_gradient_estimator = False\n",
    "n_epochs = args.get(\"n_epochs\", 1000)\n",
    "batch_size = args.get(\"batch_size\", 1024*10)\n",
    "n_posterior_samples_total = args.get(\"n_posterior_samples_total\", 128)\n",
    "\n",
    "optimizer = args.get(\"optimizer\", \"adam\")\n",
    "optimizer_lr = args.get(\"optimizer_lr\", 0.001)\n",
    "early_stopping_n_iters = args.get(\"early_stopping_n_iters\", 100)\n",
    "\n",
    "beta = args.get(\"beta\", 0.4)\n",
    "beta_annealing = args.get(\"beta_annealing\", False)\n",
    "beta_annealing_iterations = args.get(\"beta_annealing_iterations\", 200)\n",
    "\n",
    "report_every_n_iterations = args.get(\"report_every_n_iterations\", 200)\n",
    "eval_n_samples = args.get(\"eval_n_samples\", 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_noise_scale = args.get(\"observation_noise_scale\", 0.1)\n",
    "learn_observation_noise_scale = args.get(\"learn_observation_noise_scale\", False)\n",
    "\n",
    "\n",
    "prior_fit_result = args.get(\"prior\", \n",
    "    \"../results/experiment_house_electric_fit_prior_varying_lengthscales_20250116183912/house_electric_ls1_nnsilu_s1_results.json.gz\"\n",
    "    # \"../results/experiment_house_electric_fit_prior_varying_lengthscales_20250116183912/house_electric_ls1_fourier2_s0_results.json.gz\"\n",
    "    )\n",
    "\n",
    "datafile = \"../data/uci_household_power_consumption/individual+household+electric+power+consumption.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_prefix = args.get(\"run_name\", \"house_electric_fit_posterior_vi\")+\"_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args[\"smoke_test\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.get(\"smoke_test\", False):\n",
    "    nf_model = 1\n",
    "    n_epochs = 4\n",
    "    report_every_n_iterations = 2\n",
    "    n_posterior_samples_total = 3\n",
    "    eval_n_samples = 12\n",
    "    batch_size = 1024*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration = {'__name__': '__main__', '__doc__': 'Automatically created module for IPython interactive environment', '__package__': None, '__loader__': None, '__spec__': None, '__builtin__': <module 'builtins' (built-in)>, '__builtins__': <module 'builtins' (built-in)>, '_ih': ['', \"get_ipython().run_line_magic('reload_ext', 'autoreload')\\nget_ipython().run_line_magic('autoreload', '2')\", 'import sys, logging\\n\\nlogging.basicConfig(\\n        level=logging.INFO,\\n        stream=sys.stdout,\\n        format=\"[%(asctime)s] {%(pathname)s:%(lineno)d} %(levelname)s - %(message)s\",\\n        force=True,\\n    )', 'import gc\\n\\nfrom scipy.stats import norm\\nimport math\\n\\nimport numpy as np\\nimport pandas as pd\\nimport torch\\n\\n\\nfrom tqdm import tqdm\\nimport time', 'if torch.cuda.is_available():\\n    torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\\n    # torch.set_default_tensor_type(\"torch.cuda.DoubleTensor\")\\n    device = \"cuda\"\\n\\nelse:\\n    torch.set_default_tensor_type(\"torch.FloatTensor\")\\n    device = \"cpu\"', '# torch.set_default_tensor_type(\"torch.FloatTensor\")\\n# device = \"cpu\"', 'print(f\"Used device = {device}\")', 'sys.path.append(\"../\")\\n\\nimport reparameterized\\nfrom reparameterized import sampling', 'from data.minibatches import generate_minibatches, iterate_over_minibatches\\nfrom aux.gpu import gpu_memory_info_smi\\n\\nimport import_export', 'import args as args_loading\\n\\n# used when run from command line\\nargs = args_loading.parse_args()\\n# print(f\"Parsing args = {args}\")', 'random_seed = args.get(\"random_seed\", 0)', '# NF \\nnf_model = args.get(\"nf_model\", 0) \\nrealnvp_rezero = args.get(\"realnvp_rezero\", False)\\n\\n# NF posterior training\\nstl_gradient_estimator = False\\nn_epochs = args.get(\"n_epochs\", 1000)\\nbatch_size = args.get(\"batch_size\", 1024*10)\\nn_posterior_samples_total = args.get(\"n_posterior_samples_total\", 128)\\n\\noptimizer = args.get(\"optimizer\", \"adam\")\\noptimizer_lr = args.get(\"optimizer_lr\", 0.001)\\nearly_stopping_n_iters = args.get(\"early_stopping_n_iters\", 100)\\n\\nbeta = args.get(\"beta\", 0.4)\\nbeta_annealing = args.get(\"beta_annealing\", False)\\nbeta_annealing_iterations = args.get(\"beta_annealing_iterations\", 200)\\n\\nreport_every_n_iterations = args.get(\"report_every_n_iterations\", 200)\\neval_n_samples = args.get(\"eval_n_samples\", 1024)', 'observation_noise_scale = args.get(\"observation_noise_scale\", 0.1)\\nlearn_observation_noise_scale = args.get(\"learn_observation_noise_scale\", False)\\n\\n\\nprior_fit_result = args.get(\"prior\", \\n    \"../results/experiment_house_electric_fit_prior_varying_lengthscales_20250116183912/house_electric_ls1_nnsilu_s1_results.json.gz\"\\n    # \"../results/experiment_house_electric_fit_prior_varying_lengthscales_20250116183912/house_electric_ls1_fourier2_s0_results.json.gz\"\\n    )\\n\\ndatafile = \"../data/uci_household_power_consumption/individual+household+electric+power+consumption.npy\"', 'output_prefix = args.get(\"run_name\", \"house_electric_fit_posterior_vi\")+\"_\"', 'args[\"smoke_test\"] = True', 'if args.get(\"smoke_test\", False):\\n    nf_model = 1\\n    n_epochs = 4\\n    report_every_n_iterations = 2\\n    n_posterior_samples_total = 3\\n    eval_n_samples = 12\\n    batch_size = 1024*10', 'print(f\"Configuration = {locals()}\")'], '_oh': {}, '_dh': [PosixPath('/home/kusmierc/bnn-from-gp/bnngp/notebooks')], 'In': ['', \"get_ipython().run_line_magic('reload_ext', 'autoreload')\\nget_ipython().run_line_magic('autoreload', '2')\", 'import sys, logging\\n\\nlogging.basicConfig(\\n        level=logging.INFO,\\n        stream=sys.stdout,\\n        format=\"[%(asctime)s] {%(pathname)s:%(lineno)d} %(levelname)s - %(message)s\",\\n        force=True,\\n    )', 'import gc\\n\\nfrom scipy.stats import norm\\nimport math\\n\\nimport numpy as np\\nimport pandas as pd\\nimport torch\\n\\n\\nfrom tqdm import tqdm\\nimport time', 'if torch.cuda.is_available():\\n    torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\\n    # torch.set_default_tensor_type(\"torch.cuda.DoubleTensor\")\\n    device = \"cuda\"\\n\\nelse:\\n    torch.set_default_tensor_type(\"torch.FloatTensor\")\\n    device = \"cpu\"', '# torch.set_default_tensor_type(\"torch.FloatTensor\")\\n# device = \"cpu\"', 'print(f\"Used device = {device}\")', 'sys.path.append(\"../\")\\n\\nimport reparameterized\\nfrom reparameterized import sampling', 'from data.minibatches import generate_minibatches, iterate_over_minibatches\\nfrom aux.gpu import gpu_memory_info_smi\\n\\nimport import_export', 'import args as args_loading\\n\\n# used when run from command line\\nargs = args_loading.parse_args()\\n# print(f\"Parsing args = {args}\")', 'random_seed = args.get(\"random_seed\", 0)', '# NF \\nnf_model = args.get(\"nf_model\", 0) \\nrealnvp_rezero = args.get(\"realnvp_rezero\", False)\\n\\n# NF posterior training\\nstl_gradient_estimator = False\\nn_epochs = args.get(\"n_epochs\", 1000)\\nbatch_size = args.get(\"batch_size\", 1024*10)\\nn_posterior_samples_total = args.get(\"n_posterior_samples_total\", 128)\\n\\noptimizer = args.get(\"optimizer\", \"adam\")\\noptimizer_lr = args.get(\"optimizer_lr\", 0.001)\\nearly_stopping_n_iters = args.get(\"early_stopping_n_iters\", 100)\\n\\nbeta = args.get(\"beta\", 0.4)\\nbeta_annealing = args.get(\"beta_annealing\", False)\\nbeta_annealing_iterations = args.get(\"beta_annealing_iterations\", 200)\\n\\nreport_every_n_iterations = args.get(\"report_every_n_iterations\", 200)\\neval_n_samples = args.get(\"eval_n_samples\", 1024)', 'observation_noise_scale = args.get(\"observation_noise_scale\", 0.1)\\nlearn_observation_noise_scale = args.get(\"learn_observation_noise_scale\", False)\\n\\n\\nprior_fit_result = args.get(\"prior\", \\n    \"../results/experiment_house_electric_fit_prior_varying_lengthscales_20250116183912/house_electric_ls1_nnsilu_s1_results.json.gz\"\\n    # \"../results/experiment_house_electric_fit_prior_varying_lengthscales_20250116183912/house_electric_ls1_fourier2_s0_results.json.gz\"\\n    )\\n\\ndatafile = \"../data/uci_household_power_consumption/individual+household+electric+power+consumption.npy\"', 'output_prefix = args.get(\"run_name\", \"house_electric_fit_posterior_vi\")+\"_\"', 'args[\"smoke_test\"] = True', 'if args.get(\"smoke_test\", False):\\n    nf_model = 1\\n    n_epochs = 4\\n    report_every_n_iterations = 2\\n    n_posterior_samples_total = 3\\n    eval_n_samples = 12\\n    batch_size = 1024*10', 'print(f\"Configuration = {locals()}\")'], 'Out': {}, 'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x753f810e7c70>>, 'exit': <IPython.core.autocall.ZMQExitAutocall object at 0x753f810f99d0>, 'quit': <IPython.core.autocall.ZMQExitAutocall object at 0x753f810f99d0>, 'open': <function open at 0x753f8320edc0>, '_': '', '__': '', '___': '', '__vsc_ipynb_file__': '/home/kusmierc/bnn-from-gp/bnngp/notebooks/house_electric_fit_posterior_vi.ipynb', '_i': 'if args.get(\"smoke_test\", False):\\n    nf_model = 1\\n    n_epochs = 4\\n    report_every_n_iterations = 2\\n    n_posterior_samples_total = 3\\n    eval_n_samples = 12\\n    batch_size = 1024*10', '_ii': 'args[\"smoke_test\"] = True', '_iii': 'output_prefix = args.get(\"run_name\", \"house_electric_fit_posterior_vi\")+\"_\"', '_i1': '%reload_ext autoreload\\n%autoreload 2', '_i2': 'import sys, logging\\n\\nlogging.basicConfig(\\n        level=logging.INFO,\\n        stream=sys.stdout,\\n        format=\"[%(asctime)s] {%(pathname)s:%(lineno)d} %(levelname)s - %(message)s\",\\n        force=True,\\n    )', 'sys': <module 'sys' (built-in)>, 'logging': <module 'logging' from '/home/kusmierc/miniconda3_py39/lib/python3.9/logging/__init__.py'>, '_i3': 'import gc\\n\\nfrom scipy.stats import norm\\nimport math\\n\\nimport numpy as np\\nimport pandas as pd\\nimport torch\\n\\n\\nfrom tqdm import tqdm\\nimport time', 'gc': <module 'gc' (built-in)>, 'norm': <scipy.stats._continuous_distns.norm_gen object at 0x753f6009a490>, 'math': <module 'math' from '/home/kusmierc/miniconda3_py39/lib/python3.9/lib-dynload/math.cpython-39-x86_64-linux-gnu.so'>, 'np': <module 'numpy' from '/home/kusmierc/bnn-from-gp/bnngp/.env/lib/python3.9/site-packages/numpy/__init__.py'>, 'pd': <module 'pandas' from '/home/kusmierc/bnn-from-gp/bnngp/.env/lib/python3.9/site-packages/pandas/__init__.py'>, 'torch': <module 'torch' from '/home/kusmierc/bnn-from-gp/bnngp/.env/lib/python3.9/site-packages/torch/__init__.py'>, 'tqdm': <class 'tqdm.std.tqdm'>, 'time': <module 'time' (built-in)>, '_i4': 'if torch.cuda.is_available():\\n    torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\\n    # torch.set_default_tensor_type(\"torch.cuda.DoubleTensor\")\\n    device = \"cuda\"\\n\\nelse:\\n    torch.set_default_tensor_type(\"torch.FloatTensor\")\\n    device = \"cpu\"', 'device': 'cuda', '_i5': '# torch.set_default_tensor_type(\"torch.FloatTensor\")\\n# device = \"cpu\"', '_i6': 'print(f\"Used device = {device}\")', '_i7': 'sys.path.append(\"../\")\\n\\nimport reparameterized\\nfrom reparameterized import sampling', 'reparameterized': <module 'reparameterized' from '/home/kusmierc/bnn-from-gp/bnngp/notebooks/../reparameterized/__init__.py'>, 'sampling': <module 'reparameterized.sampling' from '/home/kusmierc/bnn-from-gp/bnngp/notebooks/../reparameterized/sampling/__init__.py'>, '_i8': 'from data.minibatches import generate_minibatches, iterate_over_minibatches\\nfrom aux.gpu import gpu_memory_info_smi\\n\\nimport import_export', 'generate_minibatches': <function generate_minibatches at 0x753e9dec6700>, 'iterate_over_minibatches': <function iterate_over_minibatches at 0x753f806e5ee0>, 'gpu_memory_info_smi': <function gpu_memory_info_smi at 0x753e9dec6f70>, 'import_export': <module 'import_export' from '/home/kusmierc/bnn-from-gp/bnngp/notebooks/../import_export.py'>, '_i9': 'import args as args_loading\\n\\n# used when run from command line\\nargs = args_loading.parse_args()\\n# print(f\"Parsing args = {args}\")', 'args_loading': <module 'args' from '/home/kusmierc/bnn-from-gp/bnngp/notebooks/../args.py'>, 'args': {'f': '/home/kusmierc/.local/share/jupyter/runtime/kernel-v34dcb72d2b4dde9923cc60ac231ecef9c4bbffa90.json', 'smoke_test': True}, '_i10': 'random_seed = args.get(\"random_seed\", 0)', 'random_seed': 0, '_i11': '# NF \\nnf_model = args.get(\"nf_model\", 0) \\nrealnvp_rezero = args.get(\"realnvp_rezero\", False)\\n\\n# NF posterior training\\nstl_gradient_estimator = False\\nn_epochs = args.get(\"n_epochs\", 1000)\\nbatch_size = args.get(\"batch_size\", 1024*10)\\nn_posterior_samples_total = args.get(\"n_posterior_samples_total\", 128)\\n\\noptimizer = args.get(\"optimizer\", \"adam\")\\noptimizer_lr = args.get(\"optimizer_lr\", 0.001)\\nearly_stopping_n_iters = args.get(\"early_stopping_n_iters\", 100)\\n\\nbeta = args.get(\"beta\", 0.4)\\nbeta_annealing = args.get(\"beta_annealing\", False)\\nbeta_annealing_iterations = args.get(\"beta_annealing_iterations\", 200)\\n\\nreport_every_n_iterations = args.get(\"report_every_n_iterations\", 200)\\neval_n_samples = args.get(\"eval_n_samples\", 1024)', 'nf_model': 1, 'realnvp_rezero': False, 'stl_gradient_estimator': False, 'n_epochs': 4, 'batch_size': 10240, 'n_posterior_samples_total': 3, 'optimizer': 'adam', 'optimizer_lr': 0.001, 'early_stopping_n_iters': 100, 'beta': 0.4, 'beta_annealing': False, 'beta_annealing_iterations': 200, 'report_every_n_iterations': 2, 'eval_n_samples': 12, '_i12': 'observation_noise_scale = args.get(\"observation_noise_scale\", 0.1)\\nlearn_observation_noise_scale = args.get(\"learn_observation_noise_scale\", False)\\n\\n\\nprior_fit_result = args.get(\"prior\", \\n    \"../results/experiment_house_electric_fit_prior_varying_lengthscales_20250116183912/house_electric_ls1_nnsilu_s1_results.json.gz\"\\n    # \"../results/experiment_house_electric_fit_prior_varying_lengthscales_20250116183912/house_electric_ls1_fourier2_s0_results.json.gz\"\\n    )\\n\\ndatafile = \"../data/uci_household_power_consumption/individual+household+electric+power+consumption.npy\"', 'observation_noise_scale': 0.1, 'learn_observation_noise_scale': False, 'prior_fit_result': '../results/experiment_house_electric_fit_prior_varying_lengthscales_20250116183912/house_electric_ls1_nnsilu_s1_results.json.gz', 'datafile': '../data/uci_household_power_consumption/individual+household+electric+power+consumption.npy', '_i13': 'output_prefix = args.get(\"run_name\", \"house_electric_fit_posterior_vi\")+\"_\"', 'output_prefix': 'house_electric_fit_posterior_vi_', '_i14': 'args[\"smoke_test\"] = True', '_i15': 'if args.get(\"smoke_test\", False):\\n    nf_model = 1\\n    n_epochs = 4\\n    report_every_n_iterations = 2\\n    n_posterior_samples_total = 3\\n    eval_n_samples = 12\\n    batch_size = 1024*10', '_i16': 'print(f\"Configuration = {locals()}\")'}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Configuration = {locals()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ryh9SpsI2Pc5",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Load and preprocess data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.uci_household_power_consumption import load as data_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data torch.Size([1366186, 8]) torch.Size([1366186]) torch.Size([683094, 8]) torch.Size([683094]) tensor(9.3178) tensor(9.5873) 9d025f6b9a0637a8e6858f9ad33f3a12\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    x_test,\n",
    "    y_test,\n",
    "    y_train_r_mean,\n",
    "    y_train_r_std,\n",
    "    tt_split_hsh,\n",
    ") = data_src.prepare_data(datafile=datafile, train_prop=train_data_proportion)\n",
    "print(\"data\", x_train.shape, y_train.shape, x_test.shape, y_test.shape, y_train_r_mean, y_train_r_std, tt_split_hsh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3QfBW_Vj2Pc9",
    "outputId": "6bce1ccf-f2fd-4d02-f466-8d9cb503e086"
   },
   "outputs": [],
   "source": [
    "# Data to torch tensors for training\n",
    "training_data = x_train\n",
    "training_targets = y_train\n",
    "\n",
    "train_x = training_data.to(device)\n",
    "train_y = training_targets.to(device)\n",
    "x_test =  x_test.to(device)\n",
    "y_test =  y_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-27 18:17:35,697] {/home/kusmierc/bnn-from-gp/bnngp/notebooks/../aux/gpu.py:59} INFO - [gpu_memory_info_smi] GPU 0: Total Memory: 10240 MiB, Used: 508 MiB, Free: 9500 MiB\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "gpu_memory_info_smi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CCZvLfSX2Pc_"
   },
   "source": [
    "## BNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if learn_observation_noise_scale:\n",
    "    observation_noise_scale = torch.tensor(observation_noise_scale, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(bnn, train_x, train_y, observation_noise_scale):\n",
    "    pred_means = bnn(train_x)\n",
    "    log_probs = torch.distributions.Normal(\n",
    "        pred_means.flatten(), observation_noise_scale\n",
    "    ).log_prob(train_y.flatten())\n",
    "    return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_export \n",
    "\n",
    "prior_params = import_export.load_from_json(prior_fit_result)\n",
    "activation = prior_params[\"parameters\"][\"activation\"].to(device)\n",
    "net_width = prior_params[\"net_width\"]\n",
    "\n",
    "prior_params[\"parameters\"].pop(\"activation\")\n",
    "priors = prior_params[\"parameters\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    from matplotlib import pyplot as plt\n",
    "\n",
    "    x = torch.linspace(-40, 40, 100)\n",
    "    y = activation(x).detach().cpu().numpy()\n",
    "\n",
    "    plt.plot(x.detach().cpu().numpy(), y)\n",
    "    plt.title(\"Activation Function\")\n",
    "    plt.xlabel(\"Input\")\n",
    "    plt.ylabel(\"Output\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "turning off gradient for fci.weight\n",
      "turning off gradient for fci.bias\n",
      "turning off gradient for fc.0.weight\n",
      "turning off gradient for fc.0.bias\n",
      "turning off gradient for fco.weight\n",
      "turning off gradient for fco.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in activation.named_parameters():\n",
    "    print(f\"turning off gradient for {name}\")\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "priors = {'layer1.weight': Normal(loc: 0.0, scale: 0.4770457148551941), 'layer1.bias': Normal(loc: 0.0, scale: 0.10911737382411957), 'layer2.weight': Normal(loc: 0.0, scale: 1.5432627201080322), 'layer2.bias': Normal(loc: 0.0, scale: 0.07194221764802933)}\n"
     ]
    }
   ],
   "source": [
    "print(f\"priors = {priors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_priors(priors, samples):\n",
    "    return sum(priors[n].log_prob(p.flatten()).sum() for n, p in samples.items())  # sum over all parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_predictive(\n",
    "    bnn,\n",
    "    sampler,\n",
    "    x,\n",
    "    n_samples=1024,\n",
    "    n_samples_per_sampling=128,\n",
    "    batch_size=1024,\n",
    "    verbose=False,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\" Generate n_samples of prediction means using the sampler function.\"\"\"\n",
    "    \n",
    "    n_samples_per_sampling = min(n_samples_per_sampling, n_samples)\n",
    "    assert (\n",
    "        n_samples % n_samples_per_sampling == 0\n",
    "    ), \"n_samples_per_sampling must divide n_samples\"\n",
    "\n",
    "    preds = []\n",
    "    progress_tracking = lambda i: i\n",
    "    if verbose:\n",
    "        logging.info(\n",
    "            f\"[get_predicitive] generating n_samples={n_samples} \"\n",
    "            f\"n_samples_per_sampling={n_samples_per_sampling} \"\n",
    "            f\"minibatch_size={batch_size}\"\n",
    "        )\n",
    "        progress_tracking = tqdm\n",
    "    for _ in progress_tracking(range(n_samples // n_samples_per_sampling)):\n",
    "        samples, _ = sampler(n_samples_per_sampling)\n",
    "\n",
    "        for s in reparameterized.take_parameters_sample(samples):\n",
    "            reparameterized.load_state_dict(bnn, s)\n",
    "\n",
    "            pred = []\n",
    "            for x1 in iterate_over_minibatches(x, batch_size=batch_size):\n",
    "                pred1 = bnn(x1)\n",
    "                pred.append(pred1)\n",
    "            pred = torch.concat(pred)\n",
    "\n",
    "            preds.append(pred)\n",
    "\n",
    "    preds = torch.stack(preds)[:, :, 0]\n",
    "    assert preds.shape == torch.Size(\n",
    "        [n_samples, x.shape[0]]\n",
    "    ), f\"preds.shape={preds.shape} vs {n_samples, x.shape[0]}\"\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def negative_log_likelihood(train_y, mu_samples, observation_noise_scale, agg=None):\n",
    "    \"\"\"Calculate the negative log likelihood for the given dataset.\"\"\"\n",
    "    n_samples = mu_samples.size(0)\n",
    "    n_train = train_y.size(0)\n",
    "    \n",
    "    # Create a normal distribution with the given mean and observation noise\n",
    "    d = torch.distributions.Normal(mu_samples, observation_noise_scale)\n",
    "    \n",
    "    # Compute the log probabilities of the training data under this distribution\n",
    "    log_probs = d.log_prob(train_y.unsqueeze(0).expand(n_samples, n_train))\n",
    "    \n",
    "    # Average the log probabilities over the samples\n",
    "    avg_log_probs = torch.logsumexp(log_probs, dim=0) - math.log(n_samples)\n",
    "    \n",
    "    # Sum the -log probabilities to get the negative log likelihood\n",
    "    return -avg_log_probs if agg is None else agg(-avg_log_probs)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def negative_log_likelihood_gaussian(train_y, mu_samples, observation_noise_scale, agg=None):\n",
    "    \"\"\"Calculates the negative log likelihood of Gaussian distribution.\"\"\"\n",
    "    \n",
    "    y, mu, var = train_y, mu_samples.mean(0), mu_samples.var(0)+observation_noise_scale**2\n",
    "    \n",
    "    y, mu, var = y.squeeze(), mu.squeeze(), var.squeeze()\n",
    "    nll = -norm.logpdf(y.cpu().detach(), loc=mu.cpu().detach(), scale=np.sqrt(var.cpu().detach()))\n",
    "\n",
    "    return torch.tensor(nll) if agg is None else agg(nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_bnn(x_test, y_test, bnn, sampler, observation_noise_scale, **kwargs):\n",
    "    gc.collect()\n",
    "\n",
    "    pred_mean_samples = sample_from_predictive(bnn, sampler, x_test, **kwargs)\n",
    "    gc.collect()\n",
    "\n",
    "    nlls = []\n",
    "    nlls_g = []\n",
    "    data_size = len(y_test)\n",
    "    for pred_mean_samples1, y_test1 in iterate_over_minibatches(\n",
    "        pred_mean_samples.T, y_test, 10 * 1024\n",
    "    ):\n",
    "        pred_mean_samples1 = pred_mean_samples1.T\n",
    "        \n",
    "        nll = negative_log_likelihood(y_test1.flatten(), pred_mean_samples1, observation_noise_scale)\n",
    "        nlls.append(nll)\n",
    "                                \n",
    "        nll_g = negative_log_likelihood_gaussian(y_test1.flatten(), pred_mean_samples1, observation_noise_scale)\n",
    "        nlls_g.append(nll_g)\n",
    "\n",
    "    nlls_g = torch.cat(nlls_g)\n",
    "    assert nlls_g.shape == torch.Size([data_size])\n",
    "    nll_g = nlls_g.mean()  # average over data points\n",
    "\n",
    "    nlls = torch.cat(nlls)\n",
    "    assert nlls.shape == torch.Size([data_size])        \n",
    "    nll = nlls.mean()  # average over data points\n",
    "\n",
    "    preds_avg_mean = pred_mean_samples.mean(axis=0)\n",
    "    rmse = torch.sqrt(((preds_avg_mean - y_test) ** 2).mean())\n",
    "\n",
    "    gc.collect()\n",
    "    return {\"rmse\": rmse.item(), \"nll_g\": nll_g.item(), \"nll\": nll.item()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FvEkSHYs2Pc_"
   },
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "WH0Q4-p22Pc_"
   },
   "outputs": [],
   "source": [
    "# Network\n",
    "class SingleHiddenLayerWideNN(torch.nn.Module):\n",
    "    def __init__(self, width=1000, indim=1, outdim=1, activation=None):\n",
    "        super().__init__()\n",
    "        self.width = width\n",
    "        self.layer1 = torch.nn.Linear(indim, width)\n",
    "        self.layer2 = torch.nn.Linear(width, outdim)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x, learnable_activation=None):\n",
    "        learnable_activation = self.activation or learnable_activation\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = learnable_activation(x)\n",
    "        x = self.layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SingleHiddenLayerWideNN(\n",
      "  (layer1): Linear(in_features=8, out_features=128, bias=True)\n",
      "  (layer2): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (activation): NNActivation(\n",
      "    (activation): SiLU()\n",
      "    (fci): Linear(in_features=1, out_features=5, bias=True)\n",
      "    (fc): Sequential(\n",
      "      (0): Linear(in_features=5, out_features=5, bias=True)\n",
      "    )\n",
      "    (fco): Linear(in_features=5, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "indim = x_train.shape[-1]\n",
    "\n",
    "bnn = SingleHiddenLayerWideNN(indim=indim, width=net_width, activation=activation)\n",
    "print(bnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "id": "exHJ3Y-L2PdB",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Posterior Sampler = Normalizing Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x753f0d8f1310>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bnn_learning_params = dict_keys(['layer1.weight', 'layer1.bias', 'layer2.weight', 'layer2.bias'])\n"
     ]
    }
   ],
   "source": [
    "bnn_learning_params = {n: p for n,p in bnn.named_parameters() if \"activation\" not in n}  # exclude activation params\n",
    "print(f\"bnn_learning_params = {bnn_learning_params.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": true,
    "id": "4IDSYuHx2PdB",
    "outputId": "66bf60ad-2f0f-4916-9163-b7e6d7786c9d",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if nf_model==0:\n",
    "    sampler, variational_params, aux_objs = sampling.create_multiparameter_sampler_dict(\n",
    "        sampling.create_flow_sampler,\n",
    "        bnn_learning_params,\n",
    "        build_flow_func = sampling.realnvp.build_realnvp,\n",
    "        realnvp_rezero=realnvp_rezero,\n",
    "        realnvp_num_layers=realnvp_num_layers,\n",
    "        realnvp_m=int(realnvp_m_multiplier*net_width),\n",
    "        )\n",
    "    n_posterior_samples = n_posterior_samples_total  # in one forward operation\n",
    "    \n",
    "elif nf_model==1:\n",
    "    sampler, variational_params, aux_objs = sampling.create_multiparameter_sampler_dict(\n",
    "        sampling.create_flow_sampler,\n",
    "        bnn_learning_params,\n",
    "        build_flow_func = sampling.realnvp.build_realnvp,\n",
    "        realnvp_rezero=realnvp_rezero,\n",
    "        realnvp_num_layers=2,\n",
    "        realnvp_m=32,\n",
    "        )\n",
    "    n_posterior_samples = n_posterior_samples_total  # in one forward operation\n",
    "    \n",
    "    \n",
    "else:\n",
    "    raise ValueError(f\"Wrong value of nf_model={nf_model}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flow= RealNVP(\n",
      "  (t): ModuleList(\n",
      "    (0-1): 2 x Sequential(\n",
      "      (0): Linear(in_features=641, out_features=32, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (3): LeakyReLU(negative_slope=0.01)\n",
      "      (4): Linear(in_features=32, out_features=640, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (s): ModuleList(\n",
      "    (0-1): 2 x Sequential(\n",
      "      (0): Linear(in_features=641, out_features=32, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (3): LeakyReLU(negative_slope=0.01)\n",
      "      (4): Linear(in_features=32, out_features=640, bias=True)\n",
      "      (5): Tanh()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"flow=\", aux_objs[\"flow\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-27 18:17:38,613] {/home/kusmierc/bnn-from-gp/bnngp/notebooks/../aux/gpu.py:59} INFO - [gpu_memory_info_smi] GPU 0: Total Memory: 10240 MiB, Used: 550 MiB, Free: 9458 MiB\n"
     ]
    }
   ],
   "source": [
    "gpu_memory_info_smi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ZLm6l422PdC",
    "outputId": "5d2e0586-f8b1-4334-d11e-5654d346e9e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampled 1 sample in  0.15s\n",
      "n_parameters = 1281\n",
      "n_variational_params = 170880\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "samples, q_nlls = sampler(1)\n",
    "print(f\"sampled 1 sample in {time.time()-start_time: .2f}s\")\n",
    "\n",
    "s1 = next(reparameterized.take_parameters_sample(samples))\n",
    "n_parameters = sum(s.numel() for s in s1.values())\n",
    "print(f\"n_parameters = {n_parameters}\")\n",
    "\n",
    "n_variational_params = sum(p.numel() for p in variational_params.values())\n",
    "print(f\"n_variational_params = {n_variational_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples= {'layer1.weight': tensor([[[-0.8969,  1.1240,  2.7709,  ..., -0.3293, -1.0415, -0.7757],\n",
      "         [-0.5347, -0.6538, -0.9425,  ...,  0.2962,  1.6913,  1.0320],\n",
      "         [ 1.6295,  2.0771, -0.7510,  ...,  0.4557,  0.2408, -1.3693],\n",
      "         ...,\n",
      "         [ 0.8948,  0.7201,  0.8651,  ..., -0.6070,  0.6629, -0.7956],\n",
      "         [ 0.4405, -1.0280,  0.5489,  ..., -0.5298,  0.2224, -1.4329],\n",
      "         [-0.8330,  0.0102,  0.3885,  ..., -1.3906, -1.4269,  2.4943]]],\n",
      "       grad_fn=<ViewBackward0>), 'layer1.bias': tensor([[ 1.3380, -0.2402, -0.4335,  1.1140, -0.5176, -0.4133,  0.0951, -0.4121,\n",
      "          1.1289, -1.2984, -0.4001, -0.4176, -0.5007, -0.3035,  1.2628, -0.4590,\n",
      "         -1.0774,  2.1742,  1.9130, -0.3481,  0.7011,  0.4887,  1.8415, -0.3275,\n",
      "          0.2887, -0.7674, -0.9787, -0.9320,  0.0723,  0.7709,  2.6389, -1.6853,\n",
      "         -0.9787,  0.9988,  1.5123, -0.9125, -0.2764, -1.3456,  0.1131, -1.0530,\n",
      "          0.5553,  1.3628, -1.4490, -1.7397, -0.3691,  1.3694, -0.3849, -0.3267,\n",
      "          1.1600, -0.1199, -0.1568, -0.1746, -1.5953,  2.5746, -0.5887, -0.3017,\n",
      "          0.0042,  0.9486, -2.7364,  0.4276,  0.2691,  0.0865,  0.3458,  1.7509,\n",
      "         -2.6603,  0.2880,  1.3254, -0.4124,  0.7088, -1.8008, -0.8138, -0.2429,\n",
      "         -0.9951, -0.6963, -0.0465,  0.6930,  0.8471,  0.5310,  0.6911,  0.7783,\n",
      "         -0.5281,  0.2651, -1.2540, -1.1221,  2.3120, -0.3427, -0.0954, -0.5594,\n",
      "         -0.4365, -0.4147, -0.1102, -1.4026, -0.7483, -1.4115, -2.0852,  0.8193,\n",
      "          0.6102,  0.1880, -0.7692, -0.2295, -0.7616, -0.8452,  2.4681,  1.0647,\n",
      "          1.3903,  1.5939, -0.3747, -0.8876, -0.6967,  0.8825,  1.8054,  0.2241,\n",
      "          0.2894, -0.7256,  1.4144,  0.2954,  1.3479,  0.2790,  0.1743,  1.3037,\n",
      "          0.5843, -0.2073, -0.2196,  0.6770, -0.2266,  0.4785, -0.5741, -1.2113]],\n",
      "       grad_fn=<ViewBackward0>), 'layer2.weight': tensor([[[-1.1556,  0.3191,  0.6294,  0.6989,  1.0134,  2.5101,  0.9790,\n",
      "          -0.1192, -0.2445,  0.4760, -1.2418, -0.5255,  0.2768,  0.4022,\n",
      "           0.4879,  0.1141,  0.4207, -1.0964, -0.3380,  0.6565, -1.0085,\n",
      "           0.1827,  0.9971, -0.2783,  0.0407,  1.7780, -1.1264, -0.7461,\n",
      "          -0.7015,  2.4028, -0.4541, -0.1490, -1.8153,  1.5953, -0.8615,\n",
      "          -0.0321,  0.6526, -0.5890, -0.5959,  1.2799, -1.3788, -0.5390,\n",
      "           0.7372,  0.6115,  0.0126, -0.0903,  0.6698, -1.0448, -1.6252,\n",
      "          -1.5047, -0.6221,  1.3678, -0.4264, -2.7729,  1.5536,  1.0350,\n",
      "          -1.4174,  1.8717, -0.4504, -0.8478, -0.7454, -3.4361,  1.0802,\n",
      "           1.9549,  0.2866,  1.6965,  0.1303, -0.7405, -0.1382, -0.8445,\n",
      "          -0.7286, -1.1881,  0.1948, -0.6917, -2.0757, -1.0376, -1.2852,\n",
      "           0.5152,  0.5777, -0.1799, -0.9603,  2.2201,  0.8408,  0.9227,\n",
      "          -1.9804,  0.7791,  0.4800, -1.0842, -1.4657,  0.2710,  1.2429,\n",
      "          -0.0502,  0.6427,  1.6681,  1.1812,  0.1043,  0.5270, -1.3777,\n",
      "           1.9607,  1.6639, -0.6145, -0.4901, -0.1482, -0.0602,  0.2960,\n",
      "          -0.6990, -0.1693, -0.6664, -0.9475,  0.1720,  0.8117, -0.2503,\n",
      "          -0.5381, -0.5922,  1.7117, -1.0369, -1.1647, -0.9919, -0.1917,\n",
      "           0.5762, -0.2177, -0.1553,  0.3266,  0.8383,  0.8388,  1.4664,\n",
      "          -1.2410, -0.0455]]], grad_fn=<ViewBackward0>), 'layer2.bias': tensor([[-1.2400]], grad_fn=<ViewBackward0>)} q_nlls= tensor([1822.2367], grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"samples=\", samples, \"q_nlls=\", q_nlls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7CNU30fG8Jwr",
    "outputId": "bc6f95e3-0caf-4e56-89d7-ed85bc58999c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: samples=cuda:0 train_x=cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using device: samples={next(iter(samples.values())).device} train_x={train_x.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-27 18:17:39,222] {/home/kusmierc/bnn-from-gp/bnngp/notebooks/../aux/gpu.py:59} INFO - [gpu_memory_info_smi] GPU 0: Total Memory: 10240 MiB, Used: 550 MiB, Free: 9458 MiB\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "gpu_memory_info_smi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "S09vaJEp4T1Y"
   },
   "outputs": [],
   "source": [
    "# start_time = time.time()\n",
    "# n_samples = 20\n",
    "# samples, q_nlls = sampler(n_samples)\n",
    "# print(f\"sampled {n_samples} samples in {time.time()-start_time: .2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training ELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 957
    },
    "id": "yPVoy_kZ2PdC",
    "outputId": "b7c616f0-8ffb-467d-e885-2d3704b50964"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-27 18:17:39,484] {/tmp/ipykernel_1578376/3923916588.py:21} INFO - [get_predicitive] generating n_samples=12 n_samples_per_sampling=12 minibatch_size=1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:01<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-27 18:17:41,492] {/tmp/ipykernel_1578376/3150869581.py:11} INFO - Before traininging: rmse: 1.2783 nll_g: 1.6789 nll: 7.0422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1578376/2848490842.py:27: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  nll = -norm.logpdf(y.cpu().detach(), loc=mu.cpu().detach(), scale=np.sqrt(var.cpu().detach()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-27 18:17:41,659] {/home/kusmierc/bnn-from-gp/bnngp/notebooks/../aux/gpu.py:59} INFO - [gpu_memory_info_smi] GPU 0: Total Memory: 10240 MiB, Used: 562 MiB, Free: 9446 MiB\n"
     ]
    }
   ],
   "source": [
    "eval_stats = eval_bnn(\n",
    "    x_test,\n",
    "    y_test,\n",
    "    bnn,\n",
    "    sampler,\n",
    "    n_samples=eval_n_samples,\n",
    "    observation_noise_scale=observation_noise_scale,\n",
    "    verbose=True,\n",
    ")\n",
    "eval_stats_str = \" \".join(f\"{n}: {v:.4f}\" for n, v in eval_stats.items())\n",
    "logging.info(f\"Before traininging: {eval_stats_str}\")\n",
    "gc.collect()\n",
    "gpu_memory_info_smi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_optimizer(opt, *args, **kwargs):\n",
    "    optimizers = {\n",
    "        'sgd': torch.optim.SGD(*args, **kwargs),\n",
    "        'asgd': torch.optim.ASGD(*args, **kwargs),\n",
    "        'adam': torch.optim.Adam(*args, **kwargs),\n",
    "        'adamw': torch.optim.AdamW(*args, **kwargs),\n",
    "        'adamax': torch.optim.Adamax(*args, **kwargs),\n",
    "        'rms': torch.optim.RMSprop(*args, **kwargs),\n",
    "        'rprop': torch.optim.Rprop(*args, **kwargs),\n",
    "        # 'adah': AdaHessian(*args, **kwargs)\n",
    "    }\n",
    "    return optimizers[opt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "editable": true,
    "id": "af1cpbmm2PdC",
    "outputId": "c8a9e297-2e13-433f-d5bb-beb6d7c4a9fb",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-27 18:17:47,712] {/home/kusmierc/bnn-from-gp/bnngp/notebooks/../data/minibatches.py:51} INFO - [generate_minibatches] Dropping batch of size=4266 (min required=6144)! \n",
      "[2025-01-27 18:17:49,905] {/tmp/ipykernel_1578376/3615354928.py:139} INFO - [ 7s] epoch=0: loss= 751551.44 (log_lik=-747746.96 KLD= 9511.18)  test: rmse: 1.0297 nll_g: 1.8813 nll: 21.7333  obs_noise: 0.100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1578376/2848490842.py:27: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  nll = -norm.logpdf(y.cpu().detach(), loc=mu.cpu().detach(), scale=np.sqrt(var.cpu().detach()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-27 18:17:49,985] {/home/kusmierc/bnn-from-gp/bnngp/notebooks/../aux/gpu.py:59} INFO - [gpu_memory_info_smi] GPU 0: Total Memory: 10240 MiB, Used: 676 MiB, Free: 9332 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|       | 1/4 [00:07<00:22,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-27 18:17:55,130] {/home/kusmierc/bnn-from-gp/bnngp/notebooks/../data/minibatches.py:51} INFO - [generate_minibatches] Dropping batch of size=4266 (min required=6144)! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|     | 2/4 [00:12<00:12,  6.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-27 18:18:00,585] {/home/kusmierc/bnn-from-gp/bnngp/notebooks/../data/minibatches.py:51} INFO - [generate_minibatches] Dropping batch of size=4266 (min required=6144)! \n",
      "[2025-01-27 18:18:02,708] {/tmp/ipykernel_1578376/3615354928.py:139} INFO - [ 20s] epoch=2: loss= 575468.55 (log_lik=-571504.49 KLD= 9910.15)  test: rmse: 1.0032 nll_g: 4.0328 nll: 30.5949  obs_noise: 0.100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1578376/2848490842.py:27: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  nll = -norm.logpdf(y.cpu().detach(), loc=mu.cpu().detach(), scale=np.sqrt(var.cpu().detach()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-27 18:18:02,786] {/home/kusmierc/bnn-from-gp/bnngp/notebooks/../aux/gpu.py:59} INFO - [gpu_memory_info_smi] GPU 0: Total Memory: 10240 MiB, Used: 676 MiB, Free: 9332 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|  | 3/4 [00:20<00:06,  6.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-27 18:18:08,102] {/home/kusmierc/bnn-from-gp/bnngp/notebooks/../data/minibatches.py:51} INFO - [generate_minibatches] Dropping batch of size=4266 (min required=6144)! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 4/4 [00:25<00:00,  6.47s/it]\n"
     ]
    }
   ],
   "source": [
    "optimized_parameters = list(variational_params.values())\n",
    "if learn_observation_noise_scale:\n",
    "    optimized_parameters += [observation_noise_scale]\n",
    "optimizer = pick_optimizer(optimizer, optimized_parameters, lr=optimizer_lr)\n",
    "\n",
    "assert (\n",
    "    n_posterior_samples_total % n_posterior_samples == 0\n",
    "), \"n_posterior_samples must divide n_posterior_samples_total\"\n",
    "\n",
    "start_time = time.time()\n",
    "history = [{\"epoch\": -1, \"time\": 0}]\n",
    "for n, v in eval_stats.items():\n",
    "    history[-1][f\"test_{n}\"] = v    \n",
    "best_loss, best_it = (\n",
    "    float(\"inf\"),\n",
    "    -early_stopping_n_iters,\n",
    ")  # early stopping initialization\n",
    "beta_max = beta\n",
    "for it in tqdm(range(n_epochs)):\n",
    "    ###################################################################################################\n",
    "\n",
    "    KLDs, log_liks, losses = [], [], []\n",
    "    for minibatch_no, (minibatch_x, minibatch_y) in enumerate(\n",
    "        generate_minibatches(\n",
    "            train_x,\n",
    "            train_y,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            last_batch_size_required=0.6,\n",
    "        )\n",
    "    ):  ###################################################################################################\n",
    "        # print(f\"[{time.time()-start_time:.1f}] next minibatch of size = {len(minibatch_x)}\")\n",
    "        # gc.collect()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if beta_annealing:\n",
    "            beta = np.min([beta_max, 0.001 + it / beta_annealing_iterations])\n",
    "\n",
    "        loss_vi = 0.0\n",
    "        for s in range(\n",
    "            n_posterior_samples_total // n_posterior_samples\n",
    "        ):  # gradient accumulation steps\n",
    "            #################################################################################################\n",
    "            # print(f\"[{time.time()-start_time:.1f}] epoch={it} accumulating_step={s}\")\n",
    "\n",
    "            samples, q_nlls = sampler(n_samples=n_posterior_samples)\n",
    "            # assert not q_nlls.isnan().any(), q_nlls\n",
    "\n",
    "            # Apply `stop_gradient` to q_nlls to eliminate the score function term\n",
    "            if stl_gradient_estimator:  # @TODO - check if this is correct\n",
    "                with torch.no_grad():\n",
    "                    q_nlls = q_nlls.clone().detach()\n",
    "\n",
    "            p_nlls = [\n",
    "                -log_priors(priors, s)\n",
    "                for s in reparameterized.take_parameters_sample(samples)\n",
    "            ]\n",
    "            p_nlls = torch.stack(p_nlls)\n",
    "\n",
    "            assert p_nlls.shape == q_nlls.shape\n",
    "            KLD = p_nlls - q_nlls\n",
    "            KLD = KLD.sum() / n_posterior_samples  # average over n_posterior_samples\n",
    "\n",
    "            log_lik = 0.0\n",
    "            for i, s in enumerate(reparameterized.take_parameters_sample(samples)):\n",
    "                # if i==0 or i==99:\n",
    "                #     print(f\"[{time.time()-start_time:.1f}] sample={i} bnn={bnn.layer1.weight.device} minibatch_x={minibatch_x.device}\")\n",
    "                reparameterized.load_state_dict(bnn, s)\n",
    "                ll = log_likelihood(\n",
    "                    bnn,\n",
    "                    minibatch_x,\n",
    "                    minibatch_y,\n",
    "                    observation_noise_scale=observation_noise_scale,\n",
    "                )\n",
    "                log_lik += ll.sum()  # sum over data\n",
    "            log_lik /= n_posterior_samples  # average over n_posterior_samples\n",
    "\n",
    "            loss_vi += -(log_lik - beta * KLD)\n",
    "\n",
    "            KLDs.append(KLD.detach().cpu().item())\n",
    "            log_liks.append(log_lik.detach().cpu().item())\n",
    "            losses.append(loss_vi.detach().cpu().item())\n",
    "        # /FOR POSTERIOR SAMPLES ###########################################################################\n",
    "\n",
    "        # print(f\"[{time.time()-start_time:.1f}] backward step\")\n",
    "        loss_vi.backward()\n",
    "        optimizer.step()\n",
    "    # /FOR MINIBATCH ###########################################################################\n",
    "\n",
    "    history.append(\n",
    "        {\n",
    "            \"epoch\": it,\n",
    "            \"time\": time.time() - start_time,\n",
    "            \"train_log_lik\": np.mean(log_liks),\n",
    "            \"train_kld\": np.mean(KLDs),\n",
    "            \"train_loss\": np.mean(losses),\n",
    "            \"beta\": beta,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"n_posterior_samples_total\": n_posterior_samples_total,\n",
    "            \"minibatch_no\": minibatch_no,\n",
    "            \"observation_noise_scale\": float(observation_noise_scale),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    # early stopping\n",
    "    improvement = False\n",
    "    if not beta_annealing or it > beta_annealing_iterations:\n",
    "        loss = np.mean(losses)\n",
    "        if loss < best_loss:\n",
    "            best_loss, best_it = loss, it\n",
    "            improvement = True\n",
    "\n",
    "        if it > best_it + early_stopping_n_iters:\n",
    "            logging.info(\n",
    "                f\"[train_nfm] Early stopping due to no improvement in {early_stopping_n_iters} iterations. \"\n",
    "                f\"best_loss={best_loss}, current loss={loss}.\"\n",
    "            )\n",
    "            break\n",
    "\n",
    "    # reporting\n",
    "    if (\n",
    "        (it < 100 and it % 10 == 0)\n",
    "        or (it > early_stopping_n_iters // 4 and improvement)\n",
    "        or it % report_every_n_iterations == 0\n",
    "    ):\n",
    "        log_lik, KLD, loss = np.mean(log_liks), np.mean(KLDs), np.mean(losses)\n",
    "        res_str = f\"loss={loss: .2f} (log_lik={log_lik: .2f} KLD={KLD: .2f})\"\n",
    "        eval_stats = eval_bnn(\n",
    "            x_test,\n",
    "            y_test,\n",
    "            bnn,\n",
    "            sampler,\n",
    "            n_samples=eval_n_samples,\n",
    "            observation_noise_scale=observation_noise_scale,\n",
    "        )\n",
    "        eval_stats_str = \" \".join(f\"{n}: {v:.4f}\" for n, v in eval_stats.items())\n",
    "        logging.info(\n",
    "            f\"[{time.time()-start_time: .0f}s] epoch={it}: {res_str}  test: {eval_stats_str}  obs_noise: {float(observation_noise_scale):.3f}\"\n",
    "        )\n",
    "\n",
    "        history[-1][\"train_log_lik\"] = log_lik\n",
    "        history[-1][\"train_kld\"] = KLD\n",
    "        history[-1][\"train_loss\"] = loss\n",
    "        for n, v in eval_stats.items():\n",
    "            history[-1][f\"test_{n}\"] = v    \n",
    "    \n",
    "        pd.DataFrame(history).to_csv(\n",
    "            output_prefix + \"results.csv\", index=False, header=True, sep=\",\"\n",
    "        )\n",
    "\n",
    "        gpu_memory_info_smi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-27 18:18:09,788] {/tmp/ipykernel_1578376/3923916588.py:21} INFO - [get_predicitive] generating n_samples=12 n_samples_per_sampling=12 minibatch_size=1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1/1 [00:01<00:00,  1.56s/it]\n",
      "/tmp/ipykernel_1578376/2848490842.py:27: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  nll = -norm.logpdf(y.cpu().detach(), loc=mu.cpu().detach(), scale=np.sqrt(var.cpu().detach()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-27 18:18:11,663] {/tmp/ipykernel_1578376/3385917661.py:11} INFO - Final after training: rmse: 0.8036 nll_g: 2.8208 nll: 17.6617\n"
     ]
    }
   ],
   "source": [
    "eval_stats = eval_bnn(\n",
    "    x_test,\n",
    "    y_test,\n",
    "    bnn,\n",
    "    sampler,\n",
    "    n_samples=eval_n_samples,\n",
    "    observation_noise_scale=observation_noise_scale,\n",
    "    verbose=True,\n",
    ")\n",
    "eval_stats_str = \" \".join(f\"{n}: {v:.4f}\" for n, v in eval_stats.items())\n",
    "logging.info(f\"Final after training: {eval_stats_str}\")\n",
    "\n",
    "history.append({\"epoch\": n_epochs, \"time\": float(\"inf\")})\n",
    "for n, v in eval_stats.items():\n",
    "    history[-1][f\"test_{n}\"] = v    \n",
    "\n",
    "pd.DataFrame(history).to_csv(\n",
    "    output_prefix + \"results.csv\", index=False, header=True, sep=\",\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-01-27 18:18:11,719] {/tmp/ipykernel_1578376/557769382.py:1} INFO - Storing fit to house_electric_fit_posterior_vi_results.json\n",
      "[2025-01-27 18:18:11,720] {/home/kusmierc/bnn-from-gp/bnngp/notebooks/../import_export.py:56} INFO - [import_export] Failed to dump <function create_multiparameter_sampler_dict.<locals>._sampler_dict_wrapper at 0x753e6ebf2670> to JSON (Object of type function is not JSON serializable). Pickling instead.\n",
      "[2025-01-27 18:18:11,721] {/home/kusmierc/bnn-from-gp/bnngp/notebooks/../import_export.py:63} WARNING - [import_export] Failed to pickle <function create_multiparameter_sampler_dict.<locals>._sampler_dict_wrapper at 0x753e6ebf2670> (Can't pickle local object 'create_multiparameter_sampler_dict.<locals>._sampler_dict_wrapper'). The object will not be recoverable.\n",
      "[2025-01-27 18:18:11,725] {/home/kusmierc/bnn-from-gp/bnngp/notebooks/../import_export.py:56} INFO - [import_export] Failed to dump Parameter containing:\n",
      "tensor([[-0.0077,  0.0041, -0.0179,  ..., -0.0184, -0.0191, -0.0086],\n",
      "        [ 0.0385,  0.0306, -0.0250,  ...,  0.0147, -0.0077,  0.0061],\n",
      "        [ 0.0336, -0.0410, -0.0158,  ...,  0.0371,  0.0187, -0.0080],\n",
      "        ...,\n",
      "        [ 0.0265, -0.0055, -0.0211,  ...,  0.0073, -0.0098,  0.0313],\n",
      "        [ 0.0432,  0.0401,  0.0300,  ...,  0.0186,  0.0400,  0.0226],\n",
      "        [ 0.0201, -0.0506,  0.0168,  ...,  0.0026, -0.0205,  0.0067]],\n",
      "       requires_grad=True) to JSON (Object of type Parameter is not JSON serializable). Pickling instead.\n",
      "[2025-01-27 18:18:11,730] {/home/kusmierc/bnn-from-gp/bnngp/notebooks/../import_export.py:56} INFO - [import_export] Failed to dump Parameter containing:\n",
      "tensor([ 0.0277,  0.0318, -0.0121, -0.0131, -0.0325,  0.0528,  0.0259,  0.0129,\n",
      "        -0.0460, -0.0029, -0.0261,  0.0049, -0.0104, -0.0014, -0.0136,  0.0254,\n",
      "         0.0236, -0.0390, -0.0098, -0.0386,  0.0159,  0.0092,  0.0259,  0.0148,\n",
      "        -0.0168, -0.0120, -0.0389,  0.0361, -0.0168, -0.0176, -0.0120,  0.0239],\n",
      "       requires_grad=True) to JSON (Object of type Parameter is not JSON serializable). Pickling instead.\n",
      "[2025-01-27 18:18:11,734] {/home/kusmierc/bnn-from-gp/bnngp/notebooks/../import_export.py:56} INFO - [import_export] Failed to dump Parameter containing:\n",
      "tensor([[-0.1737,  0.1236,  0.1310,  ...,  0.0942,  0.1599, -0.1243],\n",
      "        [-0.0546, -0.1297,  0.0090,  ..., -0.1342,  0.1462, -0.1025],\n",
      "        [ 0.0321,  0.0438, -0.1915,  ...,  0.0641,  0.1121,  0.0032],\n",
      "        ...,\n",
      "        [ 0.0959,  0.1190, -0.1076,  ..., -0.1747, -0.0480,  0.0693],\n",
      "        [-0.0943, -0.0362, -0.1074,  ...,  0.0037, -0.1135, -0.0354],\n",
      "        [ 0.0048,  0.0990,  0.1430,  ...,  0.1328, -0.1511, -0.0775]],\n",
      "       requires_grad=True) to JSON (Object of type Parameter is not JSON serializable). Pickling instead.\n",
      "[2025-01-27 18:18:11,738] {/home/kusmierc/bnn-from-gp/bnngp/notebooks/../import_export.py:56} INFO - [import_export] Failed to dump Parameter containing:\n",
      "tensor([ 0.1516, -0.0335, -0.0966,  0.0913,  0.0542,  0.0656, -0.0925,  0.1653,\n",
      "        -0.0241,  0.0274, -0.0064, -0.1054,  0.1279,  0.0461,  0.0659,  0.0607,\n",
      "        -0.0468, -0.0220,  0.1372, -0.1074, -0.1683,  0.1430,  0.0260,  0.2609,\n",
      "        -0.1700, -0.0061, -0.1690,  0.0428, -0.0537,  0.0749,  0.1214, -0.1294],\n",
      "       requires_grad=True) to JSON (Object of type Parameter is not JSON serializable). Pickling instead.\n",
      "[2025-01-27 18:18:11,742] {/home/kusmierc/bnn-from-gp/bnngp/notebooks/../import_export.py:56} INFO - [import_export] Failed to dump Parameter containing:\n",
      "tensor([[ 0.1717,  0.1700, -0.1599,  ..., -0.0780, -0.0854, -0.1004],\n",
      "        [ 0.0126,  0.1329,  0.0092,  ..., -0.0267, -0.1556,  0.1667],\n",
      "        [-0.1003,  0.0850, -0.1713,  ..., -0.0898,  0.1463,  0.1717],\n",
      "        ...,\n",
      "        [ 0.0775, -0.1283, -0.1681,  ..., -0.1227, -0.0265,  0.1247],\n",
      "        [ 0.0702, -0.0450, -0.0402,  ..., -0.0188, -0.0954, -0.1715],\n",
      "        [-0.0168,  0.0082,  0.1369,  ..., -0.1422, -0.0126, -0.0055]],\n",
      "       requires_grad=True) to JSON (Object of type Parameter is not JSON serializable). Pickling instead.\n",
      "[2025-01-27 18:18:11,753] {/home/kusmierc/bnn-from-gp/bnngp/notebooks/../import_export.py:56} INFO - [import_export] Failed to dump Parameter containing:\n",
      "tensor([-0.0214,  0.1302,  0.1668, -0.0793, -0.1049,  0.0257,  0.1516, -0.0032,\n",
      "        -0.1354,  0.0698, -0.0405,  0.0429,  0.0321, -0.0197, -0.1571,  0.0977,\n",
      "        -0.1513, -0.1132,  0.0787, -0.1604,  0.1430,  0.1004,  0.1253, -0.0561,\n",
      "        -0.1667, -0.0230,  0.1397, -0.1426,  0.0283,  0.0168,  0.1260, -0.1116,\n",
      "        -0.0706,  0.1320, -0.1592,  0.1168,  0.0654, -0.0124,  0.0801, -0.0439,\n",
      "         0.0834, -0.0247, -0.0469, -0.1047,  0.0441,  0.0297,  0.1709, -0.0774,\n",
      "        -0.0877,  0.0040,  0.0721,  0.1319,  0.0028, -0.0438,  0.0629,  0.0710,\n",
      "        -0.0334,  0.1252,  0.0720, -0.1088,  0.1493,  0.1075, -0.0661, -0.1296,\n",
      "         0.1240,  0.0927, -0.0906, -0.1411, -0.1438,  0.1668,  0.1556, -0.0118,\n",
      "         0.0179,  0.0668, -0.1036, -0.1200, -0.0385, -0.1341, -0.0737,  0.0951,\n",
      "         0.1530, -0.1175,  0.0668, -0.0233,  0.1423,  0.0732,  0.1285,  0.0099,\n",
      "         0.1104,  0.0125, -0.0828,  0.1161,  0.0144, -0.1380,  0.0778, -0.0264,\n",
      "        -0.1404, -0.1815,  0.1595, -0.1214, -0.0664,  0.1044,  0.1382,  0.0762,\n",
      "        -0.0587, -0.1036, -0.1316, -0.0973,  0.0426,  0.1427, -0.1792, -0.0523,\n",
      "        -0.1433,  0.0349, -0.0780,  0.1106, -0.0004,  0.1754,  0.0400,  0.0990,\n",
      "         0.0607, -0.1524, -0.0737, -0.1265, -0.0431,  0.1418, -0.1061,  0.1494,\n",
      "        -0.0791,  0.0793,  0.1297, -0.1452,  0.1072,  0.1332, -0.0282,  0.1791,\n",
      "        -0.0306,  0.1541,  0.1563,  0.0992, -0.0403,  0.0417,  0.1843, -0.1214,\n",
      "        -0.0740,  0.1390, -0.1362,  0.1730,  0.0800,  0.1551, -0.0135,  0.0760,\n",
      "        -0.1180,  0.0133, -0.1334,  0.1546,  0.1723,  0.0867,  0.0183, -0.1084,\n",
      "         0.1561,  0.0407, -0.1543,  0.0684,  0.0140,  0.1413,  0.1103, -0.1492,\n",
      "         0.1654,  0.0140,  0.0822, -0.1441, -0.0111,  0.1476, -0.1487, -0.1693,\n",
      "         0.0680,  0.1245, -0.1089,  0.0676,  0.1322,  0.0992, -0.1286,  0.1233,\n",
      "         0.0979,  0.1417, -0.0226, -0.1251,  0.1420,  0.0543,  0.1522, -0.0412,\n",
      "         0.1667, -0.1025, -0.1293, -0.0063, -0.1570,  0.0586, -0.0465, -0.0026,\n",
      "        -0.0422, -0.1212, -0.0496,  0.1532,  0.1146,  0.1670,  0.0822,  0.0380,\n",
      "         0.0131, -0.1771, -0.0443,  0.1045,  0.0556,  0.1462,  0.0551,  0.1176,\n",
      "         0.0757, -0.0062, -0.1618, -0.0930,  0.0241,  0.0011,  0.0595, -0.1153,\n",
      "         0.1397, -0.0982, -0.1325,  0.0555, -0.0861, -0.0605, -0.1679,  0.0315,\n",
      "        -0.1670,  0.1527, -0.0689,  0.0811,  0.0747, -0.0812,  0.0377, -0.0658,\n",
      "        -0.0788, -0.1408,  0.1437,  0.0857,  0.0851, -0.0635,  0.1191, -0.0284,\n",
      "         0.1632,  0.1556, -0.0222,  0.1008,  0.1512,  0.1409,  0.0852, -0.1414,\n",
      "        -0.0896, -0.1505, -0.1613,  0.1291,  0.0372,  0.1712, -0.1113,  0.1051,\n",
      "        -0.0718,  0.0566,  0.1623, -0.0044,  0.0365, -0.0202, -0.1241,  0.1169,\n",
      "        -0.1319,  0.1761, -0.1067,  0.0254, -0.0468, -0.0021, -0.0716, -0.0806,\n",
      "        -0.0828, -0.0689,  0.0340,  0.0166, -0.1273, -0.1282, -0.0080, -0.0878,\n",
      "         0.1086,  0.1319,  0.0679, -0.0601, -0.1342,  0.0768,  0.1466, -0.0207,\n",
      "        -0.1448,  0.0566,  0.0552, -0.0879, -0.1626, -0.0974,  0.0751,  0.0190,\n",
      "        -0.0427, -0.0743,  0.0778, -0.1761,  0.1528,  0.0696, -0.0362,  0.1593,\n",
      "        -0.1473,  0.0439, -0.0119, -0.1475, -0.1638,  0.0577,  0.0185, -0.0743,\n",
      "        -0.1460, -0.1417,  0.1226, -0.0818,  0.1320,  0.1681,  0.1414, -0.0093,\n",
      "         0.1368,  0.1604,  0.0446,  0.1156, -0.1498,  0.1627,  0.1352, -0.0385,\n",
      "         0.0575, -0.0953,  0.1506, -0.0950,  0.0006,  0.0610, -0.0486, -0.1359,\n",
      "        -0.0781, -0.0596,  0.0106, -0.1316, -0.0035,  0.1565,  0.0481, -0.0183,\n",
      "         0.0479, -0.1160, -0.0705,  0.0261,  0.0193,  0.1108,  0.1572, -0.0861,\n",
      "         0.0473, -0.2211, -0.0810, -0.0454,  0.1870, -0.1471,  0.0656,  0.0397,\n",
      "        -0.0148, -0.0305, -0.1136, -0.1125,  0.1249, -0.1090,  0.1525, -0.0082,\n",
      "         0.0569,  0.0128,  0.1254,  0.1616,  0.0475,  0.0362, -0.0016,  0.0036,\n",
      "         0.1840, -0.0694, -0.0017,  0.0500, -0.0803, -0.0006,  0.0931,  0.1425,\n",
      "         0.1635, -0.0315,  0.1886,  0.1235,  0.0925, -0.0470,  0.0962, -0.1569,\n",
      "         0.0503, -0.0462, -0.0733,  0.1703,  0.0081, -0.0173, -0.1344, -0.0670,\n",
      "         0.0333, -0.0412,  0.0386, -0.1154,  0.1350, -0.0846,  0.1053,  0.0034,\n",
      "        -0.0014, -0.1061, -0.0969,  0.0473, -0.1412,  0.0799, -0.0124,  0.0700,\n",
      "        -0.0339,  0.1592, -0.1404,  0.0009,  0.0452,  0.1280,  0.0246, -0.1043,\n",
      "         0.1260,  0.1117, -0.0249, -0.1533, -0.0706,  0.1090, -0.1723,  0.1121,\n",
      "        -0.1289,  0.0024, -0.0022, -0.0923, -0.1545,  0.1302,  0.0629,  0.1709,\n",
      "         0.1028,  0.1678,  0.0987, -0.0196, -0.1676, -0.1240, -0.0490, -0.1647,\n",
      "         0.0480, -0.0467, -0.0355,  0.0440,  0.1429,  0.0191,  0.1787,  0.0901,\n",
      "         0.0365, -0.0203, -0.0880,  0.1157,  0.1410,  0.1670, -0.0668, -0.0820,\n",
      "        -0.1318, -0.1385, -0.0457, -0.0269,  0.0944, -0.1176,  0.0935, -0.1178,\n",
      "        -0.0561,  0.0370,  0.0053, -0.1763, -0.0761,  0.0797,  0.0369,  0.0731,\n",
      "         0.1332, -0.1155, -0.0905,  0.0070,  0.1881,  0.0910, -0.1087,  0.0884,\n",
      "         0.0396, -0.1561, -0.1528,  0.0605,  0.0724,  0.1669, -0.1251, -0.1304,\n",
      "        -0.1645, -0.0518,  0.0320, -0.0274,  0.0192, -0.0919, -0.0921,  0.0596,\n",
      "        -0.1517, -0.1383, -0.1068, -0.1273,  0.1057, -0.1568, -0.1982, -0.1330,\n",
      "        -0.0373,  0.0068,  0.0600, -0.1043,  0.0385,  0.1230, -0.1318, -0.0948,\n",
      "        -0.2065, -0.0625, -0.1277,  0.0891,  0.0177,  0.0580,  0.0460, -0.0865,\n",
      "        -0.0837,  0.0490, -0.0730,  0.1654,  0.1172,  0.0594, -0.1530, -0.1097,\n",
      "        -0.1496, -0.0919,  0.1967,  0.1320, -0.1020, -0.1902, -0.0479, -0.1941,\n",
      "         0.0067, -0.1678,  0.0315, -0.0340, -0.1554,  0.1288, -0.0164,  0.0681,\n",
      "         0.1107, -0.0213, -0.2241,  0.0687, -0.0825, -0.0287, -0.0991,  0.0196,\n",
      "        -0.0026,  0.0047,  0.1323,  0.1426, -0.1797, -0.1897, -0.0591,  0.0680,\n",
      "        -0.0091, -0.1643,  0.1078,  0.1580,  0.0432, -0.1498,  0.0841, -0.2310,\n",
      "        -0.0881, -0.0398,  0.0672, -0.1580, -0.1731,  0.0574, -0.1153, -0.0258,\n",
      "         0.0296,  0.1088, -0.0182,  0.0842, -0.1723,  0.1097, -0.0897, -0.0399,\n",
      "        -0.0937, -0.3176, -0.2269, -0.3530,  0.0997, -0.0887,  0.0215, -0.1169,\n",
      "         0.0459,  0.0365, -0.1050,  0.0321, -0.1278,  0.1374, -0.0782,  0.1315,\n",
      "        -0.1255,  0.0242, -0.0269,  0.1194, -0.1730, -0.0702, -0.0127, -0.0317,\n",
      "        -0.1135,  0.0272, -0.1416,  0.0153, -0.0746, -0.0423, -0.1157, -0.0681,\n",
      "         0.0249,  0.0142,  0.0670,  0.0285, -0.1609,  0.1306,  0.1163,  0.0084],\n",
      "       requires_grad=True) to JSON (Object of type Parameter is not JSON serializable). Pickling instead.\n",
      "[2025-01-27 18:18:11,764] {/home/kusmierc/bnn-from-gp/bnngp/notebooks/../import_export.py:56} INFO - [import_export] Failed to dump Parameter containing:\n",
      "tensor([[-0.0202, -0.0339, -0.0045,  ...,  0.0022,  0.0048,  0.0248],\n",
      "        [-0.0195,  0.0320,  0.0252,  ..., -0.0384, -0.0035,  0.0462],\n",
      "        [-0.0065, -0.0548,  0.0002,  ..., -0.0261, -0.0055, -0.0207],\n",
      "        ...,\n",
      "        [-0.0255, -0.0036, -0.0165,  ...,  0.0336, -0.0530, -0.0081],\n",
      "        [ 0.0988,  0.0458,  0.0571,  ..., -0.0574,  0.0243,  0.0187],\n",
      "        [ 0.0155, -0.0084,  0.0448,  ..., -0.0164,  0.0377, -0.0233]],\n",
      "       requires_grad=True) to JSON (Object of type Parameter is not JSON serializable). Pickling instead.\n",
      "[2025-01-27 18:18:11,768] {/home/kusmierc/bnn-from-gp/bnngp/notebooks/../import_export.py:56} INFO - [import_export] Failed to dump Parameter containing:\n",
      "tensor([ 0.0698,  0.0228,  0.0014, -0.0091,  0.0473,  0.0457,  0.0417, -0.0263,\n",
      "         0.0676,  0.0084,  0.1007,  0.0572,  0.0702,  0.0497,  0.0787,  0.0612,\n",
      "         0.0326,  0.0019,  0.0549,  0.1044,  0.0887,  0.1206, -0.0148,  0.0371,\n",
      "         0.0865,  0.1115,  0.0583,  0.0419, -0.0402,  0.0744,  0.1365, -0.0715],\n",
      "       requires_grad=True) to JSON (Object of type Parameter is not JSON serializable). Pickling instead.\n",
      "[2025-01-27 18:18:11,772] {/home/kusmierc/bnn-from-gp/bnngp/notebooks/../import_export.py:56} INFO - [import_export] Failed to dump Parameter containing:\n",
      "tensor([[-0.0326,  0.1138,  0.1405,  ...,  0.0617,  0.1522, -0.0391],\n",
      "        [-0.0361, -0.1313, -0.0238,  ...,  0.0690,  0.0763, -0.0724],\n",
      "        [-0.1529, -0.1497,  0.0018,  ..., -0.1671,  0.1254,  0.0442],\n",
      "        ...,\n",
      "        [-0.0524,  0.1149, -0.1374,  ..., -0.0266,  0.0485, -0.0616],\n",
      "        [-0.1147, -0.1668, -0.0512,  ..., -0.0739,  0.0124,  0.1123],\n",
      "        [-0.0097,  0.1802, -0.0809,  ...,  0.0751, -0.1174,  0.0302]],\n",
      "       requires_grad=True) to JSON (Object of type Parameter is not JSON serializable). Pickling instead.\n",
      "[2025-01-27 18:18:11,776] {/home/kusmierc/bnn-from-gp/bnngp/notebooks/../import_export.py:56} INFO - [import_export] Failed to dump Parameter containing:\n",
      "tensor([ 0.1083,  0.2070,  0.1067,  0.0084,  0.1169,  0.0380, -0.1146,  0.2226,\n",
      "         0.2616,  0.2724, -0.0727, -0.1442, -0.0182, -0.0694,  0.2464, -0.1641,\n",
      "         0.2076,  0.0061, -0.1083,  0.0670,  0.0728,  0.1880, -0.0866,  0.1589,\n",
      "         0.2203,  0.0054,  0.2247, -0.1095,  0.1878, -0.1056, -0.1691, -0.0807],\n",
      "       requires_grad=True) to JSON (Object of type Parameter is not JSON serializable). Pickling instead.\n",
      "[2025-01-27 18:18:11,780] {/home/kusmierc/bnn-from-gp/bnngp/notebooks/../import_export.py:56} INFO - [import_export] Failed to dump Parameter containing:\n",
      "tensor([[ 0.1029,  0.0011,  0.1561,  ..., -0.0670, -0.1333,  0.0954],\n",
      "        [-0.0628, -0.0200, -0.1588,  ..., -0.1429,  0.1285,  0.0468],\n",
      "        [ 0.1383,  0.1690, -0.0910,  ...,  0.1155,  0.0137,  0.0593],\n",
      "        ...,\n",
      "        [ 0.1417,  0.1807,  0.1128,  ..., -0.0015,  0.0510,  0.0667],\n",
      "        [ 0.1640, -0.0246,  0.1228,  ...,  0.1014, -0.0498,  0.1700],\n",
      "        [ 0.1503, -0.1378, -0.0798,  ...,  0.0284,  0.1195, -0.0137]],\n",
      "       requires_grad=True) to JSON (Object of type Parameter is not JSON serializable). Pickling instead.\n",
      "[2025-01-27 18:18:11,791] {/home/kusmierc/bnn-from-gp/bnngp/notebooks/../import_export.py:56} INFO - [import_export] Failed to dump Parameter containing:\n",
      "tensor([ 0.0167, -0.1593,  0.0900, -0.0489,  0.0163, -0.1010, -0.0174, -0.1441,\n",
      "        -0.0100,  0.1820,  0.1436, -0.1637, -0.2122,  0.0853,  0.1359,  0.0052,\n",
      "         0.0986,  0.1016, -0.0094,  0.2281, -0.1662, -0.1437, -0.0099, -0.0677,\n",
      "         0.1111, -0.1369,  0.0566,  0.0521,  0.0802, -0.1071,  0.1525,  0.0178,\n",
      "        -0.0166, -0.0237,  0.1996,  0.0598, -0.1519,  0.1837,  0.0315,  0.2027,\n",
      "         0.0320, -0.1358,  0.1023,  0.2216,  0.0322,  0.1502,  0.0678,  0.1193,\n",
      "         0.0344, -0.1083, -0.0033, -0.1458, -0.1493, -0.0305, -0.0582,  0.0711,\n",
      "        -0.0987, -0.0630, -0.0029, -0.1661, -0.0962, -0.1461,  0.0580, -0.0641,\n",
      "         0.0363, -0.0892,  0.1766, -0.0077, -0.1454, -0.1909,  0.0047, -0.0847,\n",
      "         0.0323,  0.2294, -0.0207, -0.0848,  0.1105, -0.0556,  0.1558, -0.1883,\n",
      "         0.1670,  0.0836,  0.0778,  0.1371, -0.1537,  0.1294, -0.0107,  0.1875,\n",
      "         0.0147,  0.1778,  0.0364,  0.1380, -0.1303,  0.1173, -0.1561, -0.0091,\n",
      "        -0.1183,  0.0496,  0.0034, -0.0062,  0.1312, -0.1632,  0.0880,  0.0490,\n",
      "         0.1441, -0.0483, -0.1462, -0.0725,  0.1209, -0.0982, -0.1104,  0.1411,\n",
      "         0.1090,  0.1072, -0.1242,  0.1149, -0.1527, -0.0242, -0.1628, -0.1130,\n",
      "        -0.1422, -0.1685,  0.0275,  0.0280,  0.1609, -0.1675, -0.0730,  0.1053,\n",
      "         0.0916, -0.1681, -0.1797,  0.0799, -0.1336, -0.1644, -0.0060,  0.0918,\n",
      "         0.0617, -0.0775, -0.1346, -0.1763,  0.0232, -0.1699,  0.0816, -0.1575,\n",
      "        -0.0320, -0.0608,  0.0698,  0.0718, -0.1043, -0.2102,  0.0547, -0.1657,\n",
      "        -0.0290, -0.0432,  0.0719, -0.0583,  0.0564,  0.0054,  0.0604,  0.1501,\n",
      "         0.0381,  0.1155, -0.0298, -0.1220,  0.1132, -0.1379, -0.1062, -0.0721,\n",
      "        -0.0323,  0.0469,  0.0404,  0.0034, -0.0462, -0.1083, -0.0837,  0.1458,\n",
      "         0.0765,  0.0872, -0.0641, -0.1629, -0.1011, -0.1282,  0.0027, -0.0299,\n",
      "        -0.0926,  0.1796,  0.1368, -0.0417, -0.0353,  0.0536, -0.0504, -0.1452,\n",
      "        -0.0246,  0.1012, -0.2049,  0.0093,  0.1891,  0.0513, -0.0077,  0.0747,\n",
      "        -0.0941, -0.0393, -0.0968,  0.0133, -0.1282, -0.0398,  0.0519,  0.0919,\n",
      "        -0.1735, -0.2201,  0.0944, -0.0847,  0.0382, -0.1363, -0.1832,  0.0336,\n",
      "        -0.1403, -0.0684,  0.1634,  0.0743, -0.0326,  0.1641,  0.1374, -0.0672,\n",
      "        -0.1716,  0.1904,  0.1161, -0.0257,  0.1415, -0.1261,  0.0614, -0.0244,\n",
      "         0.0403, -0.1019, -0.1572,  0.1765,  0.1524,  0.1004, -0.0303, -0.0656,\n",
      "        -0.2120, -0.0856,  0.0533,  0.0885,  0.1469, -0.1724, -0.0447, -0.0045,\n",
      "         0.0197, -0.0107,  0.1347, -0.0738, -0.1102,  0.2071, -0.0407,  0.1278,\n",
      "        -0.1422, -0.1811,  0.1425,  0.0634,  0.0617, -0.1117,  0.1530,  0.1058,\n",
      "         0.0024,  0.0582,  0.1523,  0.1752,  0.1557,  0.1900,  0.0767, -0.1030,\n",
      "        -0.0387, -0.0457,  0.1241, -0.0193, -0.0674,  0.1324, -0.1462, -0.1568,\n",
      "         0.0615,  0.0695,  0.1194,  0.0206, -0.0587, -0.0447, -0.1581,  0.1268,\n",
      "        -0.0109, -0.2172, -0.1807,  0.1556, -0.1417,  0.1757,  0.1911,  0.0661,\n",
      "        -0.1577,  0.1372, -0.1992,  0.0225, -0.0125,  0.1329,  0.1189,  0.1382,\n",
      "        -0.2206, -0.0633,  0.0630,  0.2288, -0.1496, -0.0580, -0.0567,  0.0997,\n",
      "         0.0921, -0.1260, -0.0729,  0.0443,  0.0247, -0.2342, -0.1408, -0.0776,\n",
      "        -0.0636,  0.1671, -0.1291, -0.1494,  0.1685, -0.1700,  0.0247, -0.0998,\n",
      "        -0.0167,  0.1479,  0.1886,  0.2005,  0.2039,  0.1289,  0.1420,  0.0549,\n",
      "        -0.0362,  0.1274,  0.0472, -0.0999, -0.0399, -0.1057,  0.0080,  0.0127,\n",
      "         0.0802, -0.0906,  0.1000,  0.1950, -0.0052, -0.0377, -0.1980,  0.1913,\n",
      "         0.1078, -0.1642, -0.0569,  0.0247,  0.0080, -0.0393,  0.1177,  0.1255,\n",
      "         0.0234, -0.0470, -0.1731,  0.0125, -0.1270,  0.0580,  0.0307, -0.1129,\n",
      "        -0.1313, -0.0511,  0.1390, -0.1054, -0.0934, -0.0170,  0.0215,  0.0067,\n",
      "         0.1348,  0.0111, -0.1297, -0.1393, -0.0263, -0.2008, -0.0803,  0.0728,\n",
      "         0.1295, -0.0274,  0.0804, -0.1865,  0.0006, -0.2130, -0.0677, -0.1313,\n",
      "         0.0807, -0.1420,  0.1804, -0.1877, -0.2123, -0.1011,  0.0194,  0.0676,\n",
      "         0.0689, -0.0481, -0.1564,  0.2179,  0.1377, -0.1054, -0.1039,  0.1116,\n",
      "         0.1684,  0.0920,  0.1254, -0.2065, -0.0596,  0.0675, -0.0230, -0.0682,\n",
      "         0.0188, -0.0138, -0.0440, -0.1673, -0.1192, -0.0811, -0.2203,  0.1974,\n",
      "         0.0479, -0.0591,  0.2207,  0.1187,  0.0151,  0.1421,  0.0399,  0.0348,\n",
      "         0.0328,  0.0981,  0.0490, -0.0933, -0.1879, -0.1419,  0.0656, -0.0095,\n",
      "         0.1372,  0.1527, -0.1416,  0.1483, -0.1371, -0.1633, -0.1768,  0.0687,\n",
      "        -0.0612,  0.1821,  0.0509, -0.0337,  0.1140, -0.1535,  0.0268,  0.0239,\n",
      "        -0.1461,  0.1625,  0.1223,  0.0175,  0.1373,  0.1000, -0.0595, -0.0574,\n",
      "        -0.0229, -0.1434,  0.1056, -0.0198, -0.0467, -0.1299, -0.0031, -0.1392,\n",
      "        -0.0247, -0.2091,  0.0761,  0.1443,  0.1252,  0.1614,  0.1068,  0.1158,\n",
      "         0.1306,  0.1372, -0.0456, -0.1112, -0.0513,  0.0548, -0.0698,  0.0537,\n",
      "        -0.1813,  0.1241,  0.0481,  0.0481,  0.0084,  0.0430, -0.0074,  0.2171,\n",
      "        -0.1970, -0.1549, -0.1631,  0.0652, -0.1437, -0.0978, -0.0595,  0.1562,\n",
      "        -0.0350, -0.1422, -0.1403, -0.0385, -0.0860, -0.1128,  0.1239,  0.1505,\n",
      "        -0.2071,  0.1725, -0.0448,  0.1952, -0.1554, -0.0911, -0.0446,  0.0870,\n",
      "         0.0123,  0.0122,  0.0148, -0.1238,  0.1059, -0.1132, -0.0138, -0.1250,\n",
      "         0.1522, -0.1145, -0.0217,  0.2128,  0.0554,  0.0939,  0.1329,  0.0669,\n",
      "         0.0014, -0.1491,  0.1691, -0.1069, -0.1185, -0.1057, -0.0472,  0.0507,\n",
      "         0.0089,  0.0923, -0.0737,  0.1944, -0.0489, -0.0715, -0.1376, -0.1368,\n",
      "         0.0852,  0.1096, -0.0282, -0.1291,  0.1054,  0.0741,  0.1253, -0.1528,\n",
      "         0.1179,  0.0332,  0.0210, -0.1058, -0.1981,  0.0121,  0.0053, -0.0223,\n",
      "        -0.0552, -0.1977,  0.1442, -0.1345,  0.1746,  0.1204, -0.0527,  0.0809,\n",
      "        -0.1011,  0.1256,  0.1042, -0.2058, -0.0792, -0.1672,  0.0483,  0.0827,\n",
      "         0.0805,  0.1578, -0.0583, -0.1812,  0.0473, -0.0424,  0.1098,  0.0232,\n",
      "         0.0697, -0.1266,  0.0437,  0.1039, -0.1496,  0.0928, -0.1587, -0.0549,\n",
      "        -0.1529,  0.0094, -0.0524,  0.0450, -0.1345, -0.1390, -0.0286,  0.1363,\n",
      "        -0.1423,  0.0083,  0.1471, -0.0036, -0.1452,  0.0151,  0.1333,  0.0607,\n",
      "        -0.0218, -0.0319,  0.0542,  0.0262, -0.0447,  0.1751,  0.0780, -0.0965,\n",
      "         0.1842,  0.0883,  0.0577, -0.1947, -0.1991,  0.1550,  0.0875, -0.0779,\n",
      "        -0.1252, -0.0076, -0.0080,  0.0497, -0.0119, -0.1088,  0.0445,  0.0612],\n",
      "       requires_grad=True) to JSON (Object of type Parameter is not JSON serializable). Pickling instead.\n",
      "[2025-01-27 18:18:11,802] {/home/kusmierc/bnn-from-gp/bnngp/notebooks/../import_export.py:56} INFO - [import_export] Failed to dump Parameter containing:\n",
      "tensor([[ 0.0017, -0.0197,  0.0331,  ...,  0.0293, -0.0098, -0.0042],\n",
      "        [ 0.0136,  0.0159, -0.0157,  ..., -0.0124, -0.0135,  0.0188],\n",
      "        [-0.0171, -0.0414, -0.0345,  ..., -0.0205,  0.0269, -0.0022],\n",
      "        ...,\n",
      "        [ 0.0513,  0.0402, -0.0180,  ..., -0.0141,  0.0178, -0.0437],\n",
      "        [-0.0059,  0.0347,  0.0047,  ..., -0.0180,  0.0120, -0.0114],\n",
      "        [-0.0349, -0.0416, -0.0181,  ..., -0.0226, -0.0274, -0.0279]],\n",
      "       requires_grad=True) to JSON (Object of type Parameter is not JSON serializable). Pickling instead.\n",
      "[2025-01-27 18:18:11,806] {/home/kusmierc/bnn-from-gp/bnngp/notebooks/../import_export.py:56} INFO - [import_export] Failed to dump Parameter containing:\n",
      "tensor([-0.0262,  0.0472, -0.0025, -0.0048,  0.0170,  0.0167,  0.0120,  0.0138,\n",
      "         0.0564, -0.0613, -0.0068,  0.0088,  0.0459, -0.0140, -0.0187,  0.0115,\n",
      "         0.0076, -0.0179,  0.0576,  0.0064,  0.0144,  0.0123,  0.0578,  0.0889,\n",
      "         0.0242,  0.0769,  0.0611,  0.0207,  0.0205,  0.0696,  0.0350,  0.0035],\n",
      "       requires_grad=True) to JSON (Object of type Parameter is not JSON serializable). Pickling instead.\n",
      "[2025-01-27 18:18:11,810] {/home/kusmierc/bnn-from-gp/bnngp/notebooks/../import_export.py:56} INFO - [import_export] Failed to dump Parameter containing:\n",
      "tensor([[-0.1791,  0.1524,  0.1309,  ...,  0.0135,  0.1319, -0.0954],\n",
      "        [ 0.0955, -0.0454,  0.0294,  ...,  0.0276,  0.0491, -0.0859],\n",
      "        [ 0.1591,  0.1479,  0.1969,  ...,  0.1357, -0.0254, -0.1505],\n",
      "        ...,\n",
      "        [ 0.0100, -0.1054, -0.0408,  ..., -0.1132,  0.0018,  0.1478],\n",
      "        [ 0.0934, -0.0803, -0.0704,  ...,  0.0174,  0.2154, -0.0941],\n",
      "        [-0.0950,  0.0176, -0.0983,  ...,  0.0787,  0.1163, -0.1378]],\n",
      "       requires_grad=True) to JSON (Object of type Parameter is not JSON serializable). Pickling instead.\n",
      "[2025-01-27 18:18:11,813] {/home/kusmierc/bnn-from-gp/bnngp/notebooks/../import_export.py:56} INFO - [import_export] Failed to dump Parameter containing:\n",
      "tensor([ 0.0230,  0.0228,  0.0302,  0.1468,  0.0780,  0.0149, -0.0301,  0.0429,\n",
      "        -0.0431, -0.0758, -0.0885,  0.0522,  0.1513,  0.1157, -0.1306,  0.1491,\n",
      "         0.0886, -0.0442,  0.0669, -0.0490, -0.0790, -0.0265, -0.1927, -0.2194,\n",
      "        -0.0124,  0.0274,  0.1352, -0.0215, -0.1353,  0.0640, -0.1070,  0.1188],\n",
      "       requires_grad=True) to JSON (Object of type Parameter is not JSON serializable). Pickling instead.\n",
      "[2025-01-27 18:18:11,817] {/home/kusmierc/bnn-from-gp/bnngp/notebooks/../import_export.py:56} INFO - [import_export] Failed to dump Parameter containing:\n",
      "tensor([[ 0.0342,  0.0456,  0.1601,  ...,  0.0620,  0.0995,  0.1660],\n",
      "        [-0.0694, -0.0169,  0.0018,  ..., -0.0322, -0.1547,  0.0308],\n",
      "        [ 0.0556,  0.0579, -0.0787,  ...,  0.1224,  0.0936,  0.1003],\n",
      "        ...,\n",
      "        [-0.1449,  0.0808,  0.1724,  ...,  0.0916, -0.1863,  0.0718],\n",
      "        [-0.0903, -0.0999,  0.1388,  ..., -0.1874, -0.1422,  0.0017],\n",
      "        [ 0.0590,  0.1277, -0.0276,  ...,  0.1202, -0.1402, -0.1183]],\n",
      "       requires_grad=True) to JSON (Object of type Parameter is not JSON serializable). Pickling instead.\n",
      "[2025-01-27 18:18:11,829] {/home/kusmierc/bnn-from-gp/bnngp/notebooks/../import_export.py:56} INFO - [import_export] Failed to dump Parameter containing:\n",
      "tensor([-0.0183,  0.0016,  0.1740, -0.0328,  0.0969, -0.0572, -0.0009, -0.0751,\n",
      "         0.0153, -0.0464, -0.0994,  0.1229, -0.0512,  0.0569, -0.1753, -0.1089,\n",
      "         0.0166,  0.0943,  0.1172,  0.1257,  0.1694,  0.0718, -0.0323,  0.0144,\n",
      "        -0.1468,  0.1621, -0.0616, -0.1811, -0.0153, -0.0801, -0.0858,  0.0771,\n",
      "         0.1304,  0.1021, -0.1636, -0.0697, -0.0165,  0.0460, -0.1005, -0.1431,\n",
      "         0.0072,  0.0923,  0.0614,  0.0338, -0.0425, -0.2023, -0.0087,  0.0581,\n",
      "         0.0574, -0.1499,  0.0880,  0.1454,  0.1150, -0.0233,  0.1637,  0.1306,\n",
      "        -0.0528,  0.1139, -0.1402, -0.1402,  0.0569, -0.1582, -0.1205, -0.0476,\n",
      "         0.0435, -0.0620,  0.0020,  0.1412,  0.1103,  0.0097,  0.1559, -0.0550,\n",
      "         0.1467, -0.1169,  0.1349,  0.0675,  0.1603,  0.0005, -0.1121,  0.0358,\n",
      "         0.0631,  0.1033, -0.0811,  0.0475, -0.0689,  0.0891, -0.1074, -0.1796,\n",
      "         0.0391, -0.1021, -0.1764,  0.0822,  0.0119,  0.1461,  0.0390,  0.0026,\n",
      "         0.1010, -0.0059,  0.0868,  0.0381,  0.0351,  0.0216, -0.0924, -0.0442,\n",
      "         0.0120, -0.1592, -0.1079, -0.1223, -0.1022,  0.0718,  0.0682, -0.0020,\n",
      "        -0.0041,  0.0544, -0.0829, -0.0304, -0.1193, -0.1766,  0.1485,  0.1426,\n",
      "         0.0390,  0.1172, -0.1800,  0.1705, -0.1248, -0.0112,  0.0537, -0.0916,\n",
      "         0.0853,  0.0811, -0.1505,  0.0427, -0.0377,  0.1281, -0.0247,  0.0546,\n",
      "        -0.0107, -0.0452, -0.1152, -0.1479,  0.1714,  0.0353, -0.0773,  0.0456,\n",
      "         0.0454,  0.1202,  0.1221,  0.1261, -0.0540,  0.1567, -0.0757,  0.0346,\n",
      "        -0.1273,  0.0885, -0.1462, -0.1591,  0.1625,  0.1681,  0.0972,  0.0056,\n",
      "        -0.0797,  0.0647, -0.0442, -0.1190, -0.0460, -0.0261,  0.0345, -0.0696,\n",
      "         0.0111,  0.1688,  0.0542,  0.1355,  0.1166, -0.0662,  0.1054,  0.0832,\n",
      "         0.0791, -0.1617, -0.1417, -0.0409,  0.1198, -0.1414, -0.0703,  0.0022,\n",
      "        -0.0749, -0.1272,  0.0591, -0.1021,  0.0624,  0.1607,  0.0486,  0.0161,\n",
      "        -0.0439,  0.0553,  0.1315, -0.0817,  0.1296,  0.0129, -0.1598, -0.1479,\n",
      "         0.0632, -0.1608,  0.1316,  0.1073, -0.0666,  0.1008, -0.1406,  0.1428,\n",
      "        -0.1473, -0.0092, -0.0937,  0.1165, -0.1473, -0.1590, -0.1265, -0.0645,\n",
      "        -0.0655,  0.0276, -0.1140, -0.1297,  0.1384,  0.1063,  0.1383, -0.0919,\n",
      "         0.0303, -0.0145, -0.0108,  0.0376,  0.0257, -0.1343, -0.0941,  0.1180,\n",
      "         0.0018,  0.1670, -0.1560,  0.1138,  0.0933, -0.1176,  0.1624,  0.1875,\n",
      "        -0.1088,  0.0760, -0.0398, -0.1463, -0.0348,  0.1342, -0.1236, -0.0285,\n",
      "         0.0726,  0.0305,  0.0154,  0.0733, -0.0699,  0.1670,  0.1052, -0.0619,\n",
      "         0.1903, -0.1533,  0.1308,  0.0313,  0.0240, -0.0354,  0.1005, -0.1095,\n",
      "         0.1147,  0.0655, -0.1723,  0.0016,  0.0823, -0.1322,  0.1517, -0.0022,\n",
      "         0.1702, -0.0682, -0.0044, -0.0618,  0.1793, -0.1600,  0.1452,  0.0883,\n",
      "        -0.1160, -0.0775, -0.0122,  0.0336,  0.0729,  0.0932, -0.1403,  0.0297,\n",
      "        -0.1198,  0.1167,  0.0401, -0.0826,  0.1880, -0.0204, -0.0113, -0.0548,\n",
      "         0.1033, -0.0046,  0.0594,  0.0222, -0.0585, -0.1064, -0.0533,  0.0978,\n",
      "         0.0936,  0.0035, -0.0917, -0.0147, -0.1467, -0.1542, -0.1749,  0.1304,\n",
      "        -0.1608,  0.1341, -0.1225,  0.1707, -0.0525,  0.0051,  0.0834, -0.1109,\n",
      "         0.0777, -0.0097,  0.0550, -0.1649,  0.0661, -0.1492, -0.1720,  0.0977,\n",
      "        -0.0886,  0.0874,  0.1433,  0.0623,  0.1336,  0.0282,  0.0233,  0.1333,\n",
      "        -0.1331,  0.1284, -0.1253,  0.0194,  0.0635,  0.1188,  0.0732,  0.1131,\n",
      "         0.1589,  0.0643, -0.0417,  0.1592,  0.0053, -0.0961,  0.1169, -0.1151,\n",
      "         0.1170, -0.1000,  0.1145,  0.0839, -0.0486,  0.1320, -0.1661, -0.0043,\n",
      "         0.1303,  0.0835,  0.0237,  0.1011, -0.1226, -0.1547,  0.0200, -0.0951,\n",
      "         0.0034, -0.1369,  0.1005, -0.0685,  0.0896, -0.1479, -0.1529, -0.0411,\n",
      "         0.1252, -0.0383, -0.0138, -0.1867, -0.1359,  0.1148,  0.1010,  0.1018,\n",
      "        -0.1106,  0.0933, -0.0058, -0.0891,  0.1406, -0.1015, -0.1706, -0.1482,\n",
      "        -0.0478, -0.0029,  0.1285,  0.0296, -0.0286,  0.0662,  0.1027, -0.1649,\n",
      "         0.1509, -0.0012,  0.1424, -0.1181, -0.0744,  0.0806,  0.1601, -0.0830,\n",
      "         0.1031, -0.1292, -0.1643, -0.0707, -0.1891,  0.1330,  0.1502,  0.0958,\n",
      "         0.1019, -0.1088, -0.1031, -0.1480,  0.1677, -0.1712,  0.0891,  0.0897,\n",
      "         0.0970,  0.1091,  0.1159, -0.1344,  0.1452, -0.1515,  0.1080,  0.1232,\n",
      "         0.1000, -0.0788, -0.2046, -0.0740, -0.0766,  0.0790, -0.1341,  0.1299,\n",
      "        -0.0857, -0.0595,  0.0809, -0.1350,  0.1305,  0.1325, -0.0801,  0.0503,\n",
      "         0.1726, -0.0845,  0.1455, -0.0204, -0.0234,  0.1323, -0.1137, -0.1754,\n",
      "        -0.1330,  0.1297, -0.0519, -0.1717, -0.0562,  0.0445,  0.1140, -0.1774,\n",
      "        -0.1483, -0.1416, -0.1364, -0.0341, -0.1805, -0.0959, -0.0270,  0.0336,\n",
      "         0.0066, -0.0245, -0.0724, -0.1875,  0.1489,  0.1627,  0.0054, -0.0742,\n",
      "        -0.1011, -0.1225, -0.0953,  0.1191, -0.0154,  0.1590, -0.1022,  0.1596,\n",
      "        -0.1633,  0.1071, -0.0014, -0.0852,  0.0240,  0.0103, -0.0328,  0.0873,\n",
      "         0.1285,  0.0066,  0.1012, -0.1150, -0.1090, -0.0715,  0.0643,  0.0394,\n",
      "        -0.0887,  0.1055,  0.0812, -0.0227, -0.0258,  0.1073, -0.1019, -0.0294,\n",
      "        -0.2231,  0.0368, -0.1248, -0.0598,  0.0198, -0.1723, -0.1805,  0.0835,\n",
      "         0.0826, -0.0967, -0.2017, -0.1012, -0.0641, -0.0137,  0.0734,  0.0818,\n",
      "         0.0049, -0.1217, -0.1407,  0.1078, -0.1367, -0.0189,  0.0256, -0.1037,\n",
      "         0.1000, -0.0959, -0.1689, -0.0973,  0.0598, -0.0610, -0.0329, -0.1482,\n",
      "         0.1334, -0.0280, -0.0342,  0.1321, -0.1138, -0.1444,  0.0469,  0.0902,\n",
      "        -0.0163, -0.0014, -0.1926, -0.1210,  0.1211,  0.0980, -0.1409, -0.1152,\n",
      "        -0.0258,  0.1039, -0.0029,  0.0852,  0.0487, -0.0229,  0.0980,  0.1048,\n",
      "        -0.0842, -0.1089, -0.1484,  0.0442,  0.0603, -0.0957,  0.0553, -0.0961,\n",
      "        -0.1521,  0.1193,  0.1412, -0.1482, -0.1565, -0.1881,  0.0293,  0.0341,\n",
      "        -0.1829,  0.0949, -0.0106,  0.0238, -0.0548, -0.1304, -0.1380, -0.1876,\n",
      "         0.0786,  0.0856, -0.1723,  0.1090,  0.0576, -0.0173,  0.0648, -0.1027,\n",
      "        -0.0594, -0.1709,  0.0061,  0.0951,  0.0245,  0.1146, -0.0026, -0.1589,\n",
      "         0.1399,  0.1273,  0.0608,  0.1019, -0.1154, -0.1609,  0.1196, -0.1373,\n",
      "        -0.1624, -0.1315, -0.0298, -0.1370,  0.0536,  0.0837, -0.1808, -0.1829,\n",
      "         0.0762, -0.0670, -0.0855, -0.1420,  0.0813, -0.1008, -0.1685, -0.0709,\n",
      "         0.0525, -0.1774,  0.1318, -0.0469,  0.0598, -0.0400,  0.1089,  0.0986],\n",
      "       requires_grad=True) to JSON (Object of type Parameter is not JSON serializable). Pickling instead.\n",
      "[2025-01-27 18:18:11,839] {/home/kusmierc/bnn-from-gp/bnngp/notebooks/../import_export.py:56} INFO - [import_export] Failed to dump Parameter containing:\n",
      "tensor([[ 0.0141,  0.0334,  0.0188,  ..., -0.0040,  0.0057, -0.0245],\n",
      "        [ 0.0484, -0.0157, -0.0239,  ...,  0.0194,  0.0181,  0.0318],\n",
      "        [-0.0155, -0.0138,  0.0107,  ...,  0.0343, -0.0333,  0.0152],\n",
      "        ...,\n",
      "        [ 0.0379,  0.0338, -0.0415,  ..., -0.0070,  0.0314,  0.0040],\n",
      "        [ 0.0301,  0.0130, -0.0485,  ...,  0.0178,  0.0300,  0.0477],\n",
      "        [-0.0566,  0.0031,  0.0244,  ...,  0.0234, -0.0123,  0.0032]],\n",
      "       requires_grad=True) to JSON (Object of type Parameter is not JSON serializable). Pickling instead.\n",
      "[2025-01-27 18:18:11,844] {/home/kusmierc/bnn-from-gp/bnngp/notebooks/../import_export.py:56} INFO - [import_export] Failed to dump Parameter containing:\n",
      "tensor([ 0.0708, -0.0288,  0.0346,  0.0731,  0.0008,  0.0840, -0.0181,  0.0262,\n",
      "         0.0679,  0.0708,  0.0378, -0.0530,  0.1245, -0.0464,  0.0885,  0.0262,\n",
      "         0.0972,  0.0884,  0.0219,  0.0419,  0.0584, -0.0065,  0.0637,  0.0886,\n",
      "         0.0653, -0.0051, -0.0031,  0.0388, -0.0116,  0.0632,  0.0756, -0.0362],\n",
      "       requires_grad=True) to JSON (Object of type Parameter is not JSON serializable). Pickling instead.\n",
      "[2025-01-27 18:18:11,847] {/home/kusmierc/bnn-from-gp/bnngp/notebooks/../import_export.py:56} INFO - [import_export] Failed to dump Parameter containing:\n",
      "tensor([[-0.0436,  0.1640, -0.0598,  ..., -0.0495, -0.0391,  0.0196],\n",
      "        [ 0.0322,  0.0906, -0.1175,  ...,  0.0657,  0.0409, -0.0714],\n",
      "        [ 0.1602,  0.1201, -0.0497,  ...,  0.0602,  0.0039, -0.0979],\n",
      "        ...,\n",
      "        [ 0.1289,  0.0967,  0.1017,  ...,  0.0543,  0.1262,  0.1228],\n",
      "        [-0.0300,  0.0293, -0.0260,  ...,  0.0213, -0.0680, -0.0937],\n",
      "        [ 0.0624,  0.1640,  0.0166,  ..., -0.0226, -0.1332,  0.1711]],\n",
      "       requires_grad=True) to JSON (Object of type Parameter is not JSON serializable). Pickling instead.\n",
      "[2025-01-27 18:18:11,851] {/home/kusmierc/bnn-from-gp/bnngp/notebooks/../import_export.py:56} INFO - [import_export] Failed to dump Parameter containing:\n",
      "tensor([-0.0420,  0.1187,  0.1504,  0.0739,  0.1518,  0.1380,  0.0268, -0.1187,\n",
      "         0.1708,  0.0326,  0.0910, -0.1150, -0.1682, -0.0449,  0.0588,  0.1271,\n",
      "        -0.1439,  0.1136,  0.1221, -0.0447, -0.0397, -0.0987,  0.1240,  0.0876,\n",
      "        -0.0185, -0.1556, -0.0547,  0.0548, -0.0190, -0.1615,  0.0307,  0.0178],\n",
      "       requires_grad=True) to JSON (Object of type Parameter is not JSON serializable). Pickling instead.\n",
      "[2025-01-27 18:18:11,854] {/home/kusmierc/bnn-from-gp/bnngp/notebooks/../import_export.py:56} INFO - [import_export] Failed to dump Parameter containing:\n",
      "tensor([[-0.0491, -0.1580,  0.1251,  ...,  0.0276, -0.0456, -0.0980],\n",
      "        [-0.0347, -0.1206, -0.0209,  ..., -0.1421,  0.1537, -0.1427],\n",
      "        [-0.0682,  0.0228, -0.0355,  ..., -0.0995, -0.0839,  0.1753],\n",
      "        ...,\n",
      "        [-0.1509,  0.1008,  0.1531,  ...,  0.0453, -0.0085,  0.1659],\n",
      "        [-0.1269, -0.1598, -0.1172,  ..., -0.1035, -0.1251, -0.1161],\n",
      "        [-0.0171, -0.0051, -0.1484,  ...,  0.1452, -0.1528,  0.0303]],\n",
      "       requires_grad=True) to JSON (Object of type Parameter is not JSON serializable). Pickling instead.\n",
      "[2025-01-27 18:18:11,865] {/home/kusmierc/bnn-from-gp/bnngp/notebooks/../import_export.py:56} INFO - [import_export] Failed to dump Parameter containing:\n",
      "tensor([-3.2609e-02, -6.6210e-02,  7.9078e-02,  1.7259e-01,  1.3537e-01,\n",
      "        -5.2511e-02,  7.4993e-02, -5.8035e-06, -1.4608e-01, -2.4129e-02,\n",
      "        -5.0706e-02, -1.1353e-01,  7.6487e-02,  2.9832e-02,  2.1239e-02,\n",
      "         1.0620e-01, -6.6889e-02, -1.5175e-01,  1.1921e-01, -5.0187e-02,\n",
      "        -1.2913e-01,  1.3429e-01, -1.4432e-01,  1.2526e-02,  1.6599e-01,\n",
      "        -1.3842e-01,  1.0771e-01, -1.6960e-01,  1.2435e-01, -1.6390e-03,\n",
      "        -1.5265e-01,  2.6027e-02, -1.5082e-01,  1.5413e-01,  1.6043e-02,\n",
      "         4.7278e-02,  1.5706e-01, -1.3174e-01,  1.1286e-01, -1.0041e-01,\n",
      "         1.5650e-01,  1.0166e-01, -3.4119e-02,  5.2253e-02, -1.5117e-01,\n",
      "        -1.6132e-01,  9.3243e-02,  1.3423e-01,  1.2274e-01, -3.4711e-02,\n",
      "        -5.6469e-02, -1.8344e-01,  1.5865e-02,  2.2370e-02,  4.6331e-02,\n",
      "        -1.4081e-01, -1.0753e-01, -3.0561e-02, -2.2505e-01, -1.1864e-01,\n",
      "        -2.6871e-02, -2.2815e-01, -2.1410e-01, -1.4684e-01, -8.4876e-02,\n",
      "        -6.0974e-02,  1.1412e-01,  3.8499e-02, -1.7278e-01, -5.6327e-02,\n",
      "        -1.3167e-01,  8.9030e-02, -4.4407e-02,  5.7188e-02, -4.6348e-02,\n",
      "         1.2132e-01,  1.6883e-01,  1.2383e-02, -2.3931e-02, -3.8734e-02,\n",
      "         2.1278e-02, -5.2067e-02, -5.5957e-03,  6.5003e-02, -9.7317e-02,\n",
      "        -1.0899e-01,  1.3600e-01,  1.3672e-01,  5.6041e-02,  1.3100e-01,\n",
      "         9.7473e-02,  9.8585e-02, -1.5412e-01,  1.2835e-01,  5.0433e-02,\n",
      "        -1.2510e-01,  8.1124e-02, -1.4037e-01,  3.2674e-02,  9.2217e-03,\n",
      "        -9.1675e-02,  7.0727e-02, -1.5140e-01, -5.7173e-02,  9.2624e-02,\n",
      "        -8.5735e-02,  7.0637e-02, -9.6762e-02, -1.4614e-01, -1.2487e-01,\n",
      "        -1.2211e-01,  1.5503e-02,  3.7660e-03, -3.0844e-02,  1.0768e-02,\n",
      "         3.3808e-02, -8.2960e-02, -1.6113e-01, -8.3386e-03,  1.4660e-01,\n",
      "         9.7309e-02, -1.6531e-01,  1.2232e-01,  1.3512e-01,  8.1406e-02,\n",
      "        -3.6144e-02, -1.4305e-02,  2.8101e-02, -2.0185e-02, -1.0952e-01,\n",
      "        -1.1305e-01, -7.3922e-02, -2.1699e-02, -2.1067e-01, -7.6225e-02,\n",
      "        -4.7399e-02,  4.5152e-02, -6.6552e-02,  3.3502e-02, -6.0537e-02,\n",
      "         4.0338e-02,  4.8343e-03, -9.9915e-02, -1.7045e-01,  8.9405e-02,\n",
      "        -4.1837e-02,  7.4841e-02, -8.3505e-02, -1.6453e-01, -1.4191e-02,\n",
      "         1.0956e-01, -1.7537e-01,  4.7659e-02, -9.2451e-02, -8.1120e-02,\n",
      "        -1.0323e-01,  1.6309e-01, -3.1139e-02,  1.7808e-01,  4.6519e-02,\n",
      "         1.3797e-01, -1.1629e-02, -1.4132e-04,  2.1089e-02, -1.5989e-01,\n",
      "        -1.0373e-01, -2.2389e-02, -1.5931e-01,  5.7976e-02, -7.4715e-02,\n",
      "        -3.5404e-02, -3.7149e-02,  1.0576e-01, -1.3283e-01,  3.2106e-02,\n",
      "         1.3554e-01, -1.1291e-01,  6.5921e-02,  7.2831e-02, -1.2013e-01,\n",
      "         1.3662e-01,  1.9225e-02, -9.1287e-03,  1.4465e-01,  1.6181e-01,\n",
      "         6.7993e-02,  1.3754e-01, -2.2374e-02, -9.9941e-02,  3.7405e-02,\n",
      "         1.0909e-01, -4.9866e-02, -3.6910e-02,  9.4158e-02,  1.3207e-01,\n",
      "        -1.7776e-01,  7.9235e-02, -4.6484e-03, -1.3675e-01, -1.8059e-01,\n",
      "        -5.5361e-02, -8.8354e-02,  1.9329e-02, -1.5317e-01, -1.0521e-01,\n",
      "         4.9162e-02,  3.8155e-02,  6.8657e-02,  1.4778e-01, -6.4580e-02,\n",
      "         9.8266e-02,  1.4217e-01,  1.2070e-01, -9.0579e-02,  1.0433e-01,\n",
      "        -8.3425e-02,  1.2125e-01, -7.6705e-02,  1.7779e-01, -5.8628e-02,\n",
      "         6.3623e-02,  8.6039e-02,  6.1702e-02,  1.4389e-01, -5.0715e-02,\n",
      "         8.5144e-02, -1.6557e-01,  1.6531e-01, -7.4067e-02, -9.3535e-02,\n",
      "         8.0171e-02,  3.2082e-02, -1.8251e-01, -1.3432e-01,  9.8999e-03,\n",
      "         8.2286e-02,  1.0157e-01,  9.2615e-02,  7.8207e-03, -1.3409e-01,\n",
      "        -1.9890e-01, -1.0621e-01,  6.6096e-02, -6.9525e-02,  5.0799e-02,\n",
      "         1.3955e-01,  1.0627e-01, -5.5140e-02,  7.7183e-02,  1.6173e-01,\n",
      "         1.6602e-01,  1.5253e-01, -1.1439e-01,  1.2735e-01,  1.6268e-01,\n",
      "        -9.1970e-02,  7.5022e-02,  4.5603e-02,  3.6467e-02,  1.4405e-01,\n",
      "         4.6136e-02,  1.7079e-01, -7.3270e-02,  1.3307e-01, -1.7572e-01,\n",
      "        -4.5758e-02,  1.3744e-01, -1.4076e-01,  5.7489e-02, -2.9841e-02,\n",
      "         9.7678e-02, -1.0025e-01, -1.4854e-01,  1.3722e-01, -6.0490e-03,\n",
      "         1.4688e-01, -1.6356e-01,  1.1429e-01, -1.9534e-02, -9.8413e-02,\n",
      "         3.6417e-02,  1.3234e-01, -6.2708e-02,  1.0893e-01,  1.2893e-01,\n",
      "         2.6317e-02,  1.0615e-02, -6.8655e-02, -6.1647e-02, -5.2279e-02,\n",
      "        -4.0546e-02, -1.8247e-01, -1.5534e-01,  5.8580e-03, -1.5047e-01,\n",
      "        -1.0340e-01, -2.6180e-02, -1.5727e-01,  4.7308e-02,  1.2869e-01,\n",
      "         1.2724e-01, -2.0313e-02,  1.6264e-01, -4.6296e-02,  7.9413e-02,\n",
      "        -1.1106e-01, -8.0154e-02,  1.1177e-01, -3.6974e-02, -4.4576e-02,\n",
      "        -3.3328e-02, -4.3475e-02, -1.3174e-01,  1.5708e-01,  1.1183e-01,\n",
      "        -2.8821e-02,  1.1614e-01, -7.6227e-02, -1.6296e-01,  2.2646e-02,\n",
      "        -7.6494e-02, -1.4567e-01, -5.9190e-02,  1.0291e-01, -1.6468e-01,\n",
      "         2.9325e-02, -1.0415e-01, -1.0757e-01, -9.5863e-02, -9.5632e-02,\n",
      "        -2.9710e-02, -1.2936e-01,  1.1882e-01,  5.5937e-02, -1.0944e-01,\n",
      "         8.8315e-03, -6.4072e-02, -3.9871e-02, -8.3086e-02,  7.6619e-02,\n",
      "        -3.6931e-02, -1.5934e-01, -1.1786e-01,  1.2425e-01, -1.0688e-01,\n",
      "        -1.4464e-01, -5.7546e-02,  6.2308e-02, -5.4599e-02, -3.8270e-02,\n",
      "         4.3056e-02,  1.5435e-01,  1.6862e-01, -6.5293e-02, -6.7395e-02,\n",
      "         1.3847e-01, -7.4869e-02,  1.2629e-01, -8.9554e-02,  7.7234e-02,\n",
      "         7.8906e-02,  9.8050e-02, -5.5466e-02,  6.5401e-04, -2.5417e-02,\n",
      "        -1.9512e-01,  5.2259e-02,  1.4109e-01, -1.1829e-01, -1.4560e-01,\n",
      "        -1.6922e-01,  1.5225e-01, -1.6370e-01, -1.2208e-01, -2.5272e-02,\n",
      "        -1.6834e-01, -2.0587e-02, -1.0332e-01, -3.4758e-02,  6.2494e-02,\n",
      "         1.6522e-01, -1.0335e-01, -1.7383e-01, -3.3558e-02,  3.4090e-02,\n",
      "         1.4073e-01,  6.3271e-02,  7.3698e-02, -1.0629e-01,  1.8139e-01,\n",
      "         1.2402e-01,  1.5112e-02, -5.0631e-02, -1.2150e-01,  1.6665e-01,\n",
      "        -1.0432e-01, -6.0177e-02, -1.6826e-01, -8.7092e-02, -8.7756e-02,\n",
      "         1.5581e-01, -1.0523e-01,  1.1451e-01, -7.4009e-02,  1.6135e-02,\n",
      "        -6.6175e-03, -1.0599e-01,  5.3853e-02, -1.3896e-01, -9.0254e-02,\n",
      "         1.4739e-01, -2.5455e-02, -5.7360e-02,  7.4778e-02,  1.2664e-01,\n",
      "        -6.4094e-02, -1.0232e-01, -3.5596e-04, -6.9107e-02,  1.1029e-01,\n",
      "         5.6208e-03,  3.1054e-02,  2.6006e-02, -4.6226e-02,  4.4211e-02,\n",
      "        -1.1229e-01,  4.6335e-02, -1.0097e-01, -1.3093e-01, -1.0596e-01,\n",
      "         1.5122e-01, -1.3308e-01, -1.5300e-01, -1.7131e-01,  1.2671e-01,\n",
      "        -3.6585e-02,  1.7551e-01,  2.5357e-02,  6.0080e-03, -1.1706e-02,\n",
      "        -6.9313e-02,  1.4227e-01, -8.6072e-03, -8.6616e-02, -1.3562e-01,\n",
      "        -6.3260e-02, -1.2279e-02, -2.4138e-02, -1.6310e-01, -1.1269e-01,\n",
      "         5.6158e-02,  6.5127e-02,  7.9717e-02, -3.7648e-02,  1.1421e-01,\n",
      "        -1.5383e-01, -1.3135e-02, -1.6826e-01,  2.0763e-02, -1.2172e-01,\n",
      "         1.6430e-01, -4.4095e-02,  2.1759e-03,  1.1904e-01, -1.3683e-01,\n",
      "        -2.9488e-02, -5.5613e-02, -1.1591e-01, -4.7186e-02, -1.5471e-02,\n",
      "        -1.5625e-01, -1.3457e-02,  6.5439e-03, -1.1083e-01, -9.8205e-02,\n",
      "         1.7004e-01, -2.7821e-02, -1.2093e-01, -3.2987e-02,  1.4284e-01,\n",
      "        -4.4251e-02,  1.2012e-01, -3.8223e-02,  1.4571e-01, -1.7034e-01,\n",
      "        -1.6183e-01,  6.3879e-02, -1.1857e-01,  4.4412e-02,  1.1820e-01,\n",
      "        -4.0768e-02,  1.4652e-01,  1.6795e-01,  4.8269e-02, -5.9658e-02,\n",
      "         1.5889e-01,  1.1520e-01,  7.0030e-02,  1.3662e-01, -1.2787e-01,\n",
      "         2.3729e-02, -1.5576e-01, -1.6176e-01, -1.7621e-01,  3.7187e-02,\n",
      "        -4.8906e-02, -1.0243e-01,  1.1824e-01, -6.4577e-02,  4.6680e-02,\n",
      "         1.1635e-01,  4.3715e-02,  6.3678e-02, -1.0021e-01, -8.6653e-03,\n",
      "        -8.9955e-02,  1.5848e-01,  1.5482e-02,  7.9928e-04,  5.8719e-02,\n",
      "         1.4987e-01, -1.2619e-01, -5.7328e-02,  2.5141e-02, -4.4227e-02,\n",
      "        -5.3603e-02,  5.0570e-02, -1.0208e-02, -6.4005e-02,  2.9434e-02,\n",
      "        -7.7308e-02, -1.1071e-01,  3.4250e-02, -2.0036e-01, -1.9786e-02,\n",
      "         1.3944e-01, -1.6555e-01,  1.5769e-01, -4.4842e-02, -5.0672e-02,\n",
      "        -5.7081e-02, -4.2213e-02,  1.4110e-01,  1.7421e-01, -2.7725e-02,\n",
      "        -1.0548e-01, -5.3659e-02, -5.8265e-02, -1.7745e-01,  1.8138e-01,\n",
      "         2.4193e-02, -1.0087e-01,  4.1733e-02, -1.3538e-01, -2.6085e-02,\n",
      "         9.4508e-02,  1.6776e-01,  1.0910e-01,  2.6547e-02,  1.6447e-02,\n",
      "         1.1521e-02,  6.4234e-04, -6.5939e-02,  2.7845e-02,  1.0830e-01,\n",
      "        -1.3000e-01,  1.1716e-01,  1.1947e-01,  7.5897e-02,  4.3759e-02,\n",
      "         4.2776e-02, -1.2925e-01, -1.0102e-01,  9.1170e-02, -1.1175e-01,\n",
      "        -1.0850e-01,  7.1698e-02, -1.3284e-01,  3.7388e-02, -1.6374e-03,\n",
      "        -1.2870e-01, -1.4806e-01,  7.8910e-02, -1.6900e-01,  1.7744e-01,\n",
      "        -1.2377e-01, -1.6219e-01,  1.3774e-01,  9.0080e-02,  2.8871e-02,\n",
      "         2.7716e-02, -7.0652e-02,  7.8765e-02,  7.4979e-02, -7.4230e-02,\n",
      "        -1.2517e-02,  1.5898e-01, -3.9872e-02,  1.7620e-01, -1.1348e-01,\n",
      "        -1.6473e-02,  1.3649e-02,  6.9274e-02,  3.4293e-02,  1.6377e-01,\n",
      "         1.3127e-01,  3.2207e-02, -1.1664e-01, -1.6180e-01, -8.9869e-02,\n",
      "         2.0316e-02, -1.0864e-01, -5.7338e-02,  3.5574e-02, -2.5142e-01,\n",
      "        -8.6641e-03, -1.3159e-03, -1.1695e-01, -4.0062e-02,  3.8448e-02,\n",
      "        -5.3968e-02,  1.4794e-01,  1.1078e-01,  7.4324e-02, -1.4662e-01,\n",
      "        -1.6064e-01,  1.2533e-01,  8.1921e-02, -1.6347e-01,  1.3126e-01,\n",
      "         1.1131e-01,  1.8086e-01,  1.5102e-01,  1.0994e-01, -1.4414e-01,\n",
      "         1.2738e-01, -1.3912e-01, -9.5623e-05,  1.1751e-01,  1.3738e-01],\n",
      "       requires_grad=True) to JSON (Object of type Parameter is not JSON serializable). Pickling instead.\n",
      "[2025-01-27 18:18:11,876] {/home/kusmierc/bnn-from-gp/bnngp/notebooks/../import_export.py:56} INFO - [import_export] Failed to dump RealNVP(\n",
      "  (t): ModuleList(\n",
      "    (0-1): 2 x Sequential(\n",
      "      (0): Linear(in_features=641, out_features=32, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (3): LeakyReLU(negative_slope=0.01)\n",
      "      (4): Linear(in_features=32, out_features=640, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (s): ModuleList(\n",
      "    (0-1): 2 x Sequential(\n",
      "      (0): Linear(in_features=641, out_features=32, bias=True)\n",
      "      (1): LeakyReLU(negative_slope=0.01)\n",
      "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "      (3): LeakyReLU(negative_slope=0.01)\n",
      "      (4): Linear(in_features=32, out_features=640, bias=True)\n",
      "      (5): Tanh()\n",
      "    )\n",
      "  )\n",
      ") to JSON (Object of type RealNVP is not JSON serializable). Pickling instead.\n"
     ]
    }
   ],
   "source": [
    "logging.info(f\"Storing fit to {output_prefix}results.json\")\n",
    "_ = import_export.save_data_to_json(\n",
    "    output_prefix + \"results.json\",\n",
    "    results={\n",
    "        \"sampler\": sampler,\n",
    "        \"variational_params\": variational_params,\n",
    "        \"aux_objs\": aux_objs,\n",
    "        \"observation_noise_scale\": observation_noise_scale,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
